{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Embeddings II"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreComputed Embeddings: \n",
    "\n",
    "Waht if we didnt have to comput the emebddings ourselves?\n",
    "\n",
    "We are going to use a pre-trained word embedding for finding word analogies and equivalence. This exercise can be used as an Intrinsic Evaluation for the word embedding performance. In this notebook, you will apply linear algebra operations using NumPy to find analogies between words manually. This will help you to prepare for this week's assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Library for Dataframes \n",
    "import numpy as np # Library for math functions\n",
    "import pickle # Python object serialization library. Not secure\n",
    "\n",
    "word_embeddings = pickle.load( open( \"word_embeddings_subset.p\", \"rb\" ) )\n",
    "len(word_embeddings) # there should be 243 words that will be used in this assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is loaded, we can take a look at the word representations. First, note that the _word_embeddings_ is a dictionary. Each word is the key to the entry, and the value is its corresponding vector presentation. Remember that square brackets allow access to any entry if the key exists. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countryVector = word_embeddings['country'] # Get the vector representation for the word 'country'\n",
    "print(type(countryVector)) # Print the type of the vector. Note it is a numpy array\n",
    "print(countryVector) # Print the values of the vector.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that we store each vector as a NumPy array. It allows us to use the linear algebra operations on it. \n",
    "\n",
    "The vectors have a size of 300, while the vocabulary size of Google News is around 3 million words! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the vector for a given word:\n",
    "def vec(w):\n",
    "    return word_embeddings[w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operating on word embeddings\n",
    "\n",
    "Remember that understanding the data is one of the most critical steps in Data Science. Word embeddings are the result of machine learning processes and will be part of the input for further processes. These word embedding needs to be validated or at least understood because the performance of the derived model will strongly depend on its quality.\n",
    "\n",
    "Word embeddings are multidimensional arrays, usually with hundreds of attributes that pose a challenge for its interpretation. \n",
    "\n",
    "In this notebook, we will visually inspect the word embedding of some words using a pair of attributes. Raw attributes are not the best option for the creation of such charts but will allow us to illustrate the mechanical part in Python. \n",
    "\n",
    "In the next cell, we make a beautiful plot for the word embeddings of some words. Even if plotting the dots gives an idea of the words, the arrow representations help to visualize the vector's alignment as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # Import matplotlib\n",
    "\n",
    "words = ['oil', 'gas', 'happy', 'sad', 'city', 'town', 'village', 'country', 'continent', 'petroleum', 'joyful']\n",
    "\n",
    "bag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n",
    "\n",
    "col1 = 3 # Select the column for the x axis\n",
    "col2 = 2 # Select the column for the y axis\n",
    "\n",
    "# Print an arrow for each word\n",
    "for word in bag2d:\n",
    "    ax.arrow(0, 0, word[col1], word[col2], head_width=0.005, head_length=0.005, fc='r', ec='r', width = 1e-5)\n",
    "\n",
    "    \n",
    "ax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n",
    "\n",
    "# Add the word label over each dot in the scatter plot\n",
    "for i in range(0, len(words)):\n",
    "    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that similar words like 'village' and 'town' or 'petroleum', 'oil', and 'gas' tend to point in the same direction. Also, note that 'sad' and 'happy' looks close to each other; however, the vectors point in opposite directions.\n",
    "\n",
    "In this chart, one can figure out the angles and distances between the words. Some words are close in both kinds of distance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word distance\n",
    "\n",
    "Now plot the words 'sad', 'happy', 'town', and 'village'. In this same chart, display the vector from 'village' to 'town' and the vector from 'sad' to 'happy'. Let us use NumPy for these linear algebra operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['sad', 'happy', 'town', 'village']\n",
    "\n",
    "bag2d = np.array([vec(word) for word in words]) # Convert each word to its vector representation\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (10, 10)) # Create custom size image\n",
    "\n",
    "col1 = 3 # Select the column for the x axe\n",
    "col2 = 2 # Select the column for the y axe\n",
    "\n",
    "# Print an arrow for each word\n",
    "for word in bag2d:\n",
    "    ax.arrow(0, 0, word[col1], word[col2], head_width=0.0005, head_length=0.0005, fc='r', ec='r', width = 1e-5)\n",
    "    \n",
    "# print the vector difference between village and town\n",
    "village = vec('village')\n",
    "town = vec('town')\n",
    "diff = town - village\n",
    "ax.arrow(village[col1], village[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n",
    "\n",
    "# print the vector difference between village and town\n",
    "sad = vec('sad')\n",
    "happy = vec('happy')\n",
    "diff = happy - sad\n",
    "ax.arrow(sad[col1], sad[col2], diff[col1], diff[col2], fc='b', ec='b', width = 1e-5)\n",
    "\n",
    "\n",
    "ax.scatter(bag2d[:, col1], bag2d[:, col2]); # Plot a dot for each word\n",
    "\n",
    "# Add the word label over each dot in the scatter plot\n",
    "for i in range(0, len(words)):\n",
    "    ax.annotate(words[i], (bag2d[i, col1], bag2d[i, col2]))\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear algebra on word embeddings\n",
    "\n",
    "In the lectures, we saw the analogies between words using algebra on word embeddings. Let us see how to do it in Python with Numpy.\n",
    "\n",
    "To start, get the **norm** of a word in the word embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.linalg.norm(vec('town'))) # Print the norm of the word town\n",
    "print(np.linalg.norm(vec('sad'))) # Print the norm of the word sad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting capitals\n",
    "\n",
    "Now, applying vector difference and addition, one can create a vector representation for a new word. For example, we can say that the vector difference between 'France' and 'Paris' represents the concept of Capital.\n",
    "\n",
    "One can move from the city of Madrid in the direction of the concept of Capital, and obtain something close to the corresponding country to which Madrid is the Capital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "capital = vec('France') - vec('Paris')\n",
    "country = vec('Madrid') + capital\n",
    "\n",
    "print(country[0:5]) # Print the first 5 values of the vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the vector 'country' that we expected to be the same as the vector for Spain is not exactly it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = country - vec('Spain')\n",
    "print(diff[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have to look for the closest words in the embedding that matches the candidate country. If the word embedding works as expected, the most similar word must be 'Spain'. Let us define a function that helps us to do it. We will store our word embedding as a DataFrame, which facilitate the lookup operations based on the numerical vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe out of the dictionary embedding. This facilitate the algebraic operations\n",
    "keys = word_embeddings.keys()\n",
    "data = []\n",
    "for key in keys:\n",
    "    data.append(word_embeddings[key])\n",
    "\n",
    "embedding = pd.DataFrame(data=data, index=keys)\n",
    "# Define a function to find the closest word to a vector:\n",
    "def find_closest_word(v, k = 1):\n",
    "    # Calculate the vector difference from each word to the input vector\n",
    "    diff = embedding.values - v \n",
    "    # Get the norm of each difference vector. \n",
    "    # It means the squared euclidean distance from each word to the input vector\n",
    "    delta = np.sum(diff * diff, axis=1)\n",
    "    # Find the index of the minimun distance in the array\n",
    "    i = np.argmin(delta)\n",
    "    # Return the row name for this item\n",
    "    return embedding.iloc[i].name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some rows of the embedding as a Dataframe\n",
    "embedding.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us find the name that corresponds to our numerical country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_word(country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting other Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_word(vec('Italy') - vec('Rome') + vec('Madrid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_closest_word(vec('Berlin') + capital))\n",
    "print(find_closest_word(vec('Beijing') + capital))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it does not always work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_closest_word(vec('Lisbon') + capital))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Represent a sentence as a vector\n",
    "\n",
    "A whole sentence can be represented as a vector by summing all the word vectors that conform to the sentence. Let us see. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"Spain petroleum city king\"\n",
    "vdoc = [vec(x) for x in doc.split(\" \")]\n",
    "doc2vec = np.sum(vdoc, axis = 0)\n",
    "doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_closest_word(doc2vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Models\n",
    "\n",
    "What if we could donwload a model with already all the English dictionary in it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$%pip install gensim\n",
    "%pip install scipy==1.12\n",
    "\n",
    "from scipy.linalg import triu\n",
    "\n",
    "import gensim\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the model I'd like to use\n",
    "#model_nope = api.load(\"word2vec-google-news-300\")\n",
    "\n",
    "# too large, let's try something more modest\n",
    "model = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "# each word has an encoding\n",
    "model['boat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#and you can search for \"similar\" words (words that could occupy the same space)\n",
    "model.most_similar('boat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('book')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now for the jaw dropping part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can \"add\" or subtract tendencies\n",
    "model.most_similar(positive=['woman','king']\n",
    "                    #,negative=['man']\n",
    "                    ,topn=1)\n",
    "\n",
    "model.most_similar(positive=['woman','king'],negative=['man'],topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['woman','boy'],negative=['man'],topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if at first you don't succeed\n",
    "model.most_similar(positive=['grape','beer'],negative=['barley'],topn=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['lisbon','spain'],negative=['madrid'],topn=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cosine](https://www.oreilly.com/api/v2/epubs/9781788295758/files/assets/2b4a7a82-ad4c-4b2a-b808-e423a334de6f.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#why does this work?\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity([model['king']],[model['duck']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([model['king']],[model['man']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([model['king']],[model['crown']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([model['king']],[model['monarch']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that we established a baseline for similarity..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([model['king']-model['queen']],[model['son']-model['daughter']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity([model['russia']-model['moscow']],[model['poland']-model['warsaw']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Models and APIs\n",
    "(A tease for what a RAG system will be one day)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code demonstrates how to utilize embeddings to search and retrieve specific content from a document using a vector database. The document, in this case, a PDF file, is first loaded and split into manageable chunks using a text splitter. These chunks are then embedded, converting the text into high-dimensional vectors that capture semantic meaning. The embeddings are stored in a vector database, such as Chroma, allowing for efficient similarity searches. By inputting a query like \"Can I onboard data using SQL in PowerBI?\", the system retrieves the most relevant document chunks, enabling precise and efficient document-based search functionality.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document and Documentation Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_openai\n",
    "#!pip install langchain_chroma\n",
    "#!pip install langchain_chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell finished\n"
     ]
    }
   ],
   "source": [
    "## OPEN AI EMBEDDINGS:\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import os\n",
    "# import\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# Get your API KEY here: https://platform.openai.com/api-keys\n",
    "# or\n",
    "# migrate the embeddings to a model from HuggingFace\n",
    "API_KEY = '----'\n",
    "\n",
    "# Create the embeddings function\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\", api_key = API_KEY)\n",
    "\n",
    "# create a text splitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "print('Cell finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cell finished\n"
     ]
    }
   ],
   "source": [
    "# load the document and split it into chunks\n",
    "document_dir = \"./\"\n",
    "filename = \"powerbi_book.pdf\"\n",
    "file_path = os.path.join(document_dir, filename)\n",
    "\n",
    "pages = PyPDFLoader(file_path).load_and_split()\n",
    "docs = text_splitter.split_documents(pages)\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(docs, embeddings, persist_directory=\"./chroma_db\")\n",
    "print('Cell finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 C H A P T E R  4  |  Using Power BI Desktop \n",
      " Figure 4-3: Before importing data from an SQL \n",
      "database, you need to choose the loading method. \n",
      "Let’s take a moment to learn about this \n",
      "connection option because it is an important \n",
      "one and will help shed more light on how Power \n",
      "BI connections work. \n",
      "When you choose Import, Power BI Desktop \n",
      "connects to the database, loads the information, \n",
      "and stores it within its internal data model. You \n",
      "can then work on your data in Power BI Desktop \n",
      "without being connected to the database. You \n",
      "will only need a connection when you want to \n",
      "refresh the data. \n",
      "With DirectQuery, Power BI Desktop does not \n",
      "load the data into its internal database. Instead, \n",
      "it runs a query to the original database every \n",
      "time it needs to draw a chart or, in general, run a \n",
      "query. Thus, the connection between Power BI \n",
      "Desktop and the database will be permanent. \n",
      "The contrast in the query timings reflects a key \n",
      "difference: when you use Import, you are \n",
      "working with data that is only as current as the \n",
      "latest refresh, whereas with DirectQuery you \n",
      "always see the latest information available when \n",
      "you create the report.\n"
     ]
    }
   ],
   "source": [
    "# query it\n",
    "\n",
    "#user_question = 'How do I build one of those charts that look like a Swiss Cheese?'\n",
    "user_question = 'Can I onboard data using SQL in powerBi?'\n",
    "#user_question = 'Tell me about Line Charts\n",
    "\n",
    "#user_question = \"How can I do a pie chart in PowerBI?\"\n",
    "docs = db.similarity_search(user_question, k=10)\n",
    "\n",
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Content:\n",
      "137 C H A P T E R  4  |  Using Power BI Desktop \n",
      " Figure 4-3: Before importing data from an SQL \n",
      "database, you need to choose the loading method. \n",
      "Let’s take a moment to learn about this \n",
      "connection option because it is an important \n",
      "one and will help shed more light on how Power \n",
      "BI connections work. \n",
      "When you choose Import, Power BI Desktop \n",
      "connects to the database, loads the information, \n",
      "and stores it within its internal data model. You \n",
      "can then work on your data in Power BI Desktop \n",
      "without being connected to the database. You \n",
      "will only need a connection when you want to \n",
      "refresh the data. \n",
      "With DirectQuery, Power BI Desktop does not \n",
      "load the data into its internal database. Instead, \n",
      "it runs a query to the original database every \n",
      "time it needs to draw a chart or, in general, run a \n",
      "query. Thus, the connection between Power BI \n",
      "Desktop and the database will be permanent. \n",
      "The contrast in the query timings reflects a key \n",
      "difference: when you use Import, you are \n",
      "working with data that is only as current as the \n",
      "latest refresh, whereas with DirectQuery you \n",
      "always see the latest information available when \n",
      "you create the report.\n",
      "\n",
      "\n",
      "Content:\n",
      "330 C H A P T E R  8  |  Using Microsoft Power BI in your  \n",
      " company \n",
      " Using relational databases \n",
      "on-premises \n",
      "When you create a data model in Power BI \n",
      "Desktop, you often get data from an on-\n",
      "premises relational database. For example, \n",
      "Chapter 4 demonstrates how to create a new \n",
      "Power BI Desktop data model that gets data \n",
      "from a Microsoft SQL Server database. In this \n",
      "case, you use a database on-premises, and you \n",
      "can choose between the Import and DirectQuery \n",
      "connection types. The former creates a copy of \n",
      "the data in the Power BI Desktop model, which \n",
      "requires a refresh in order to synchronize the \n",
      "content of the Power BI data model with the \n",
      "source database. The latter does not create a \n",
      "copy of the data; instead, Power BI generates \n",
      "queries to SQL Server every time you navigate in \n",
      "a report. \n",
      "In both cases, after you publish the Power BI \n",
      "Desktop file to the Power BI service, the refresh \n",
      "operation requires either a Personal Gateway or \n",
      "an Enterprise Gateway. If you use the Personal \n",
      "Gateway, you can only refresh datasets created \n",
      "by importing data, but you cannot use the \n",
      "DirectQuery option. To publish a Power BI\n",
      "\n",
      "\n",
      "Content:\n",
      "329 C H A P T E R  8  |  Using Microsoft Power BI in your  \n",
      " company \n",
      " Analysis Services data source: Connect Live and \n",
      "Import Data. \n",
      "Regardless of the underlying database, when you \n",
      "create a model in Power BI by importing data, \n",
      "you have full access to the features of Power BI. \n",
      "However, you need to run or schedule a data \n",
      "refresh to keep data updated on Power BI. \n",
      "On the other hand, when you use a live \n",
      "connection, your Power BI model can have only \n",
      "one data source, so a single Power BI report \n",
      "cannot mix visualizations connected to data \n",
      "coming from different data sources. To do that, \n",
      "you must import data into Power BI. In a \n",
      "dashboard, however, you can always include \n",
      "visualizations from different reports; thus, you \n",
      "can combine visualizations from different live \n",
      "connections in a dashboard only. \n",
      "In the following sections, you will see in more \n",
      "detail how to use live and imported data sources \n",
      "using existing databases and models in your \n",
      "company.\n",
      "\n",
      "\n",
      "Content:\n",
      "336 C H A P T E R  8  |  Using Microsoft Power BI in your  \n",
      " company \n",
      "  \n",
      "Figure 8-3: Power BI can connect directly to Azure \n",
      "SQL Database and Azure SQL Data Warehouse to \n",
      "refresh a Power BI model. \n",
      "Figure 8-4 illustrates the behavior of Power BI \n",
      "using DirectQuery connected to Azure SQL \n",
      "Database or Azure SQL Data Warehouse. As with \n",
      "any DirectQuery connection, the Power BI service \n",
      "has only a semantic description of the data \n",
      "model, along with the information required to \n",
      "retrieve data from the original source database. \n",
      "It does not store a copy of data in the Microsoft \n",
      "cloud. Every time the Power BI service receives a \n",
      "query, Power BI generates one or more SQL \n",
      "queries and sends these requests to the \n",
      "relational data source, with no gateway required.\n",
      "\n",
      "\n",
      "Content:\n",
      "326 C H A P T E R  8  |  Using Microsoft Power BI in your  \n",
      " company \n",
      " Before looking at the details, here are a few \n",
      "terms with which you should be familiar: \n",
      " On-premises  If you get data from a \n",
      "database that is physically stored in a server \n",
      "managed by your company, we say that the \n",
      "database is on-premises (often shortened to \n",
      "on-prem). \n",
      " Cloud If you get data from a Microsoft \n",
      "Azure service, you are using data in the \n",
      "cloud. Cloud computing accesses and uses \n",
      "shared compute and storage resources on \n",
      "the Internet. \n",
      " Relational database This is a database \n",
      "that stores data using tables that have \n",
      "relationships with one another. Typically, \n",
      "you query this by using the SQL language. \n",
      "Examples of on-premises relational \n",
      "databases that Power BI supports are \n",
      "Microsoft SQL Server, Microsoft Access, \n",
      "Oracle, IBM DB2, MySQL, PostgreSQL, \n",
      "Sybase, and Teradata. Cloud-based relational \n",
      "databases that Power BI supports include \n",
      "Azure SQL Database and Azure SQL Data \n",
      "Warehouse. \n",
      " Rich semantic model This is a database \n",
      "that stores both data and metadata, \n",
      "simplifying navigation by using tools such as\n",
      "\n",
      "\n",
      "Content:\n",
      "333 C H A P T E R  8  |  Using Microsoft Power BI in your  \n",
      " company \n",
      " data from the original source database. Every \n",
      "time the Power BI service receives a query, it \n",
      "generates one or more queries in SQL language \n",
      "and sends these requests to the relational data \n",
      "source through the Power BI Enterprise Gateway. \n",
      "More info  The Microsoft cloud service does \n",
      "not preserve  any data received from the \n",
      "relational databases on- premises;  it might only \n",
      "keep a transient data cache on a volatile device \n",
      "in order to improve the performanc e of other \n",
      "queries sent by the same user . You can find \n",
      "more information in the Power BI Security \n",
      "whitepaper published by Microsoft at \n",
      "http://download.microsoft.com/download/4/8/\n",
      "C/48CFCF8A- 2025-4B97-B249-7B505E26E7ED/ \n",
      "Power%20BI%20Security%20Whitepaper.docx .\n",
      "\n",
      "\n",
      "Content:\n",
      "335 C H A P T E R  8  |  Using Microsoft Power BI in your  \n",
      " company \n",
      " Using relational databases in the \n",
      "cloud \n",
      "If you create a Power BI model that uses a \n",
      "relational database stored in the cloud, you \n",
      "might not need the Power BI Gateway to refresh \n",
      "data. Power BI supports direct connection to \n",
      "Azure SQL Database and Azure SQL Data \n",
      "Warehouse data sources, so you can schedule a \n",
      "data refresh or you can use DirectQuery without \n",
      "the need to install and configure any gateway. \n",
      "You will still have a different architecture, \n",
      "depending on which connection setting you use, \n",
      "Import or DirectQuery. Figure 8-3 illustrates that \n",
      "by using the Import setting you still have a copy \n",
      "of data owned by the Power BI service, but you \n",
      "can refresh that copy without any gateway if the \n",
      "data source is Azure SQL Database or Azure SQL \n",
      "Data Warehouse.\n",
      "\n",
      "\n",
      "Content:\n",
      "131 C H A P T E R  3  |  Understanding data refre sh \n",
      " from SQL Server —you need to use Power BI \n",
      "Desktop. \n",
      " With Power BI Desktop, you build models \n",
      "containing the needed information to let the \n",
      "cloud service connect to the Personal \n",
      "Gateway and retrieve the dataset. \n",
      " You can refresh your data daily with the free \n",
      "license, whereas you need a professional \n",
      "license if you need to refresh your model \n",
      "multiple times each day. \n",
      "Power BI Desktop offers many more features and \n",
      "will let you move to the next level in the learning \n",
      "path of data modeling. This is the topic for \n",
      "Chapter 4.\n",
      "\n",
      "\n",
      "Content:\n",
      "325 C H A P T E R  8  |  Using Microsoft Power BI in your  \n",
      " company \n",
      " material to understand how to use the existing \n",
      "API. If you are a business user or a BI architect, \n",
      "you will gain a fuller understanding and know \n",
      "what you can and cannot ask a developer. \n",
      "However, keep in mind that the API for Power BI \n",
      "is undergoing constant evolution. If something is \n",
      "not possible as of this writing, it might become \n",
      "possible in a future release. When in doubt, \n",
      "check what new features are available in the API. \n",
      "Getting data from \n",
      "existing systems \n",
      "Any company has a number of existing data \n",
      "sources that you can use in Power BI. You have \n",
      "seen that you can create a data model in Power \n",
      "BI by copying the content of tables that exist in \n",
      "other databases or files. You also have the \n",
      "option of refreshing this content dynamically, or \n",
      "you can directly query the data source whenever \n",
      "you access a report. By querying directly, you \n",
      "avoid the need to create a copy of the data that \n",
      "you must then synchronize periodically. In this \n",
      "section, you will see the available options with \n",
      "which you can connect Power BI to either your \n",
      "on-premises database or a database in the \n",
      "cloud.\n",
      "\n",
      "\n",
      "Content:\n",
      "334 C H A P T E R  8  |  Using Microsoft Power BI in your  \n",
      " company \n",
      "  \n",
      "Figure 8-2: For on-premises data sources, you must \n",
      "have the Power BI Enterprise Gateway for a model \n",
      "using the DirectQuery connection setting. \n",
      "As of April 2016, you can take advantage of \n",
      "DirectQuery on SQL Server, Oracle, or Teradata \n",
      "relational databases, which are all supported in \n",
      "the Power BI Enterprise Gateway.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def _get_document_prompt(docs):\n",
    "    prompt = '\\n'\n",
    "    for doc in docs:\n",
    "        prompt += '\\nContent:\\n'\n",
    "        prompt += doc.page_content + '\\n\\n'\n",
    "    return prompt\n",
    "\n",
    "print(_get_document_prompt(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
