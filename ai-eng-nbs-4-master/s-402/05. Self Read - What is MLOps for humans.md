<!-- # What's the Need for MLOps? -->

Let's understand this with an example:

For the last couple of years, there has been an increase in scams around the United States where scammers pretend to be a Tech Support Group from Microsoft, Amazon, NordVPN, etc., and scam elderly people for thousands of dollars.

Due to this, there have been a lot of complaints from residents. Authorities found that in nearly 20-30% of those cases, the calls were not actually scams but real service providers who were unsuccessful in providing their respective services due to technical difficulties, leading residents to believe it was a scam call. 

Your task is to determine whether a particular case is a scam, which will help authorities take fast action against those scammers.

Just like any Machine Learning Project, first, you collect data from the **past 3 months** from victims and authorities, where you obtain attributes such as:

- `Asked_Remote_Access`
- `Opened_Bank_Website`
- `Location_Of_Call`
- `Money_Asked`

You train a Machine Learning model to predict whether a case is a scam or not.

You deployed your model on a secure cloud, and authorities have started using it. Itâ€™s been a month, and authorities have found your ML model useful, awarding you for your brilliance!

<img src="https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTwf7yVxtTpgHZRkpeTyxB6h2NIhNSaCVspSA&usqp=CAU" width="300" height="300" alt="award" align="center"/>

But just after a few weeks, something bad happened... Authorities noticed that a lot of cases flagged as not scams turned out to be actual scams.

They investigated and found that scammers had **changed** their tactics. They started scamming people with different strategies. For instance, instead of `Opened_Bank_Website`, they now use `UPI_PAYMENTS`, and the model was not trained on this new data. 

This concept is known as **Model Drift**:
> Model drift refers to the degradation of model performance due to changes in data and relationships between input and output variables. It is relatively common for model drift to impact an organization negatively over time or sometimes suddenly.

This means you have to:
- **Collect** and **integrate** new data
- **Train** your model on the new data
- **Track** the training process
- **Experiment** with your model
- **Validate** your model
- **Deploy** your model
- **Monitor** your model

You never know when scammers will learn new strategies and your model might fail again. If it fails, you have to perform all those steps **AGAIN**. Don't worry, this is where **MLOps** comes into the picture.

**MLOps** provides an automated way to handle all these steps efficiently! It helps in:
- Model Development (Designing and Training the model on new data)
- Continuous Integration and Continuous Deployment (Deploying the model without interruptions)
- Monitoring (Checking if scammers have found new strategies)
- Validation (Validating the results obtained)

So, that's pretty much it! 

# Why MLops ? Difference of DataOps vs MLOps vs DevOps

![ds-ml/image_ops.png](https://education-team-2020.s3.eu-west-1.amazonaws.com/ds-ml/image_ops.png)

***"MLOps delivers machine learning models faster"***

*A set of practices to design, build and manage reproducible, testable and sustainable ML-powered software. 
For Big Data/Machine Learning teams, MLOps incorporate most DataOps tasks and additional ML-specific tasks, such as model versioning, testing, validation and monitoring.*

1. Compliance
In the MLOps realm, industries such as finance and healthcare often require model explainability. 

ðŸ”§ Tools: PySyft decouples private data for model training, AirCloak for data anonymisation. Awesome AI Guidelines for a curation of principals, standards and regulations around AI.

2. Iterative Development

3. Reproducibility
Machine learning models are often retrained because of either data drift. In order to reproduce the results, MLOps need to version the model and DataOps need to version the data. When being asked by an auditor which data was used to train which model to produce this specific result, the data scientist needs to be able to answer that.

ðŸ”§ Tools: experiment tracking tools, such as KubeFlow, MLFlow or SageMaker all have functionalities that link metadata to the experiment run. Pachyderm and DVC for data versioning.

4. Testing
From ML perspective, model accuracy, security, bias/fairness, interpretability all need to be tested.

ðŸ”§ Tools: libraries such as Shap & Lime for interpretability, fiddler for explainability monitoring, great expectation for data testing.

5. Continuous Deployment
There are three components to the continuous deployment of machine learning models.

- The first component is the triggering event, i.e. is the trigger a manual trigger by a data scientist, a calendar schedule event and a threshold trigger?
- The second component is the actual retraining of the new model. What are the scripts, data and hyperparameters that resulted in the model? Their versions and how they are linked to one another.
- The last component is the actual deployment of the model, which must be orchestrated by the deployment pipeline with alerting in place.

ðŸ”§ Tools: most workflow management tools have this, such as AWS SageMaker, AzureML, DataRobot, etc. Open-source tools such as Seldon, Kubeflow KFServing.

6. Automation
Automation is the core-value of DevOps, and really there are a bunch of tools specialised in different aspects of automation. Here are some resources for machine learning projects:

Awesome Machine Learning https://github.com/josephmisiti/awesome-machine-learning

Awesome Production Machine Learning https://github.com/ethicalml/awesome-production-machine-learning

7. Monitoring
Software applications need to be monitored, so does machine learning model and the data pipeline. 
For DataOps, itâ€™s important to monitor the new dataâ€™s distribution for any data and/or concept drift. On the MLOps side, in addition to model degradation, it is also paramount to monitor adversarial attacks if your model has a public API.

ðŸ”§ Tools: Most workflow management framework has some form of monitoring. Other popular tools include Prometheus for monitoring metrics, Orbit by Dessa for data & model monitoring.


***Reference:*** https://towardsdatascience.com/what-the-ops-are-you-talking-about-518b1b1a2694
