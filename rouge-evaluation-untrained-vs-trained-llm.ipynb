{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROUGE Metrics - Review\n",
    "The metrics that we have been using up to now with more traditional models, such as Accuracy, F1 Score or Recall, do not help us to evaluate the results of generative models.\n",
    "\n",
    "With these models, we are beginning to use metrics such as BLEU, ROUGE, or METEOR. Metrics that are adapted to the objective of the model.\n",
    "\n",
    "In this notebook, I want to explain how to use the ROUGE metrics to measure the quality of the summaries produced by the newest large language models.\n",
    "\n",
    "If you're looking for more detailed explanations, you can refer to the article: [Rouge Metrics: Evaluating Summaries in Large Language Models](https://medium.com/towards-artificial-intelligence/rouge-metrics-evaluating-summaries-in-large-language-models-d200ee7ca0e6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Import generic libraries\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is available on Kaggle and comprises a collection of technological news articles compiled by MIT. The article text is located in the 'Article Body' column.\n",
    "\n",
    "https://www.kaggle.com/datasets/deepanshudalal09/mit-ai-news-published-till-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "news = pd.read_csv('article.zip', index_col=0)\n",
    "DOCUMENT=\"Article Body\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Because it is just a course we select a small portion of News.\n",
    "MAX_NEWS = 3\n",
    "subset_news = news.head(MAX_NEWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Published Date</th>\n",
       "      <th>Author</th>\n",
       "      <th>Source</th>\n",
       "      <th>Article Header</th>\n",
       "      <th>Sub_Headings</th>\n",
       "      <th>Article Body</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>July 7, 2023</td>\n",
       "      <td>Adam Zewe</td>\n",
       "      <td>MIT News Office</td>\n",
       "      <td>Learning the language of molecules to predict ...</td>\n",
       "      <td>This AI system only needs a small amount of da...</td>\n",
       "      <td>['Discovering new materials and drugs typicall...</td>\n",
       "      <td>https://news.mit.edu/2023/learning-language-mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>July 6, 2023</td>\n",
       "      <td>Alex Ouyang</td>\n",
       "      <td>Abdul Latif Jameel Clinic for Machine Learning...</td>\n",
       "      <td>MIT scientists build a system that can generat...</td>\n",
       "      <td>BioAutoMATED, an open-source, automated machin...</td>\n",
       "      <td>['Is it possible to build machine-learning mod...</td>\n",
       "      <td>https://news.mit.edu/2023/bioautomated-open-so...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>June 30, 2023</td>\n",
       "      <td>Jennifer Michalowski</td>\n",
       "      <td>McGovern Institute for Brain Research</td>\n",
       "      <td>When computer vision works more like a brain, ...</td>\n",
       "      <td>Training artificial neural networks with data ...</td>\n",
       "      <td>['From cameras to self-driving cars, many of t...</td>\n",
       "      <td>https://news.mit.edu/2023/when-computer-vision...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Published Date                Author  \\\n",
       "0   July 7, 2023             Adam Zewe   \n",
       "1   July 6, 2023           Alex Ouyang   \n",
       "2  June 30, 2023  Jennifer Michalowski   \n",
       "\n",
       "                                              Source  \\\n",
       "0                                    MIT News Office   \n",
       "1  Abdul Latif Jameel Clinic for Machine Learning...   \n",
       "2              McGovern Institute for Brain Research   \n",
       "\n",
       "                                      Article Header  \\\n",
       "0  Learning the language of molecules to predict ...   \n",
       "1  MIT scientists build a system that can generat...   \n",
       "2  When computer vision works more like a brain, ...   \n",
       "\n",
       "                                        Sub_Headings  \\\n",
       "0  This AI system only needs a small amount of da...   \n",
       "1  BioAutoMATED, an open-source, automated machin...   \n",
       "2  Training artificial neural networks with data ...   \n",
       "\n",
       "                                        Article Body  \\\n",
       "0  ['Discovering new materials and drugs typicall...   \n",
       "1  ['Is it possible to build machine-learning mod...   \n",
       "2  ['From cameras to self-driving cars, many of t...   \n",
       "\n",
       "                                                 Url  \n",
       "0  https://news.mit.edu/2023/learning-language-mo...  \n",
       "1  https://news.mit.edu/2023/bioautomated-open-so...  \n",
       "2  https://news.mit.edu/2023/when-computer-vision...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_news.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "articles = subset_news[DOCUMENT].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Models and create the summaries\n",
    "\n",
    "Both models are available on Hugging Face, so we will work with the Transformers library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name_small = \"t5-base\"\n",
    "model_name_reference = \"flax-community/t5-base-cnn-dm\"\n",
    "#model_name_reference = \"pszemraj/long-t5-tglobal-base-16384-booksum-V11-big_patent-V2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#This function returns the tokenizer and the Model. \n",
    "def get_model(model_id):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
    "    \n",
    "    return tokenizer, model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eea9117071c948599c14e275d5340213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c51cd2a1dd4fd8a1a3730c170c0e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7a943b1d37b4e0ea8a1e2b32979aef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01a9759cf12c46a5a004966685b53731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "670ff38269fe4c2e94246b0a892e0650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_small, model_small = get_model(model_name_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500db91359b745818c839f2f8ff0cf63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5df9ff16a3402bbb62dfb80c4315b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2031a4464c46a6a7c67acbef150199",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f53a8afb903e41d9966b288b97b767b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c2fd0bde0b94ec5933ba61e67a645b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer_reference, model_reference = get_model(model_name_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With both models downloaded and ready, we create a function that will perform the summaries.\n",
    "\n",
    "The function takes fourth parameters:\n",
    "\n",
    "* the list of texts to summarize.\n",
    "* the tokenizer.\n",
    "* the model.\n",
    "* the maximum length for the generated summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_summaries(texts_list, tokenizer, model, max_l=125):\n",
    "    \n",
    "    # We are going to add a prefix to each article to be summarized \n",
    "    # so that the model knows what it should do\n",
    "    prefix = \"Summarize this news: \"  \n",
    "    summaries_list = [] #Will contain all summaries\n",
    "\n",
    "    texts_list = [prefix + text for text in texts_list]\n",
    "    \n",
    "    for text in texts_list:\n",
    "        \n",
    "        summary=\"\"\n",
    "        \n",
    "        #calculate the encodings\n",
    "        input_encodings = tokenizer(text, \n",
    "                                    max_length=1024, \n",
    "                                    return_tensors='pt', \n",
    "                                    padding=True, \n",
    "                                    truncation=True)\n",
    "\n",
    "        # Generate summaries\n",
    "        with torch.no_grad():\n",
    "            output = model.generate(\n",
    "                input_ids=input_encodings.input_ids,\n",
    "                attention_mask=input_encodings.attention_mask,\n",
    "                max_length=max_l,  # Set the maximum length of the generated summary\n",
    "                num_beams=2,     # Set the number of beams for beam search\n",
    "                early_stopping=True\n",
    "            )\n",
    "            \n",
    "        #Decode to get the text\n",
    "        summary = tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        \n",
    "        #Add the summary to summaries list \n",
    "        summaries_list += summary\n",
    "    return summaries_list \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the summaries, we call the 'create_summaries' function, passing both the news articles and the corresponding tokenizer and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating the summaries for both models. \n",
    "summaries_small = create_summaries(articles, \n",
    "                                  tokenizer_small, \n",
    "                                  model_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summaries_reference = create_summaries(articles, \n",
    "                                      tokenizer_reference, \n",
    "                                      model_reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MIT and MIT-Watson AI Lab have developed a unified framework. the system can simultaneously predict molecular properties and generate new molecules. it uses this grammar to construct viable molecules and predict their properties.',\n",
       " '\\'BioAutoMATED\\' is an automated machine-learning system that can select and build an appropriate model for a given dataset. it can even take care of the laborious task of data preprocessing, whittling down a months-long process to just a few hours. \\'\"We want to lower these barriers for a lot of folks that want to use machine learning or biology,\" says first co-author Jacqueline Valeri.',\n",
       " \"MIT and IBM research scientists have made a computer vision model more robust by training it to work like a part of the brain that humans and other primates rely on for object recognition. 'we asked the artificial neural network to make the function of one of your inside simulated “neural” layers as similar as possible to the corresponding biological neural layer,' says MIT professor.\"]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Researchers created a machine-learning system that automatically learns the \"language\" of molecules using only a small, domain-specific dataset. The system learns to construct viable molecules and predict their properties. Computational design and Fabrication Group will be presented at the International Conference for Machine Learning.',\n",
       " \"Automated machine-learning system can select and build an appropriate model for a given dataset. 'BioAutoMATED' is an automated machine-learning system. The tool includes binary classification models, multi-class classification models, and more complex neural networks.\",\n",
       " \"MIT and IBM researchers have found that artificial neural networks resemble the multilayered brain circuits that process visual information in humans and other primates. 'We asked it to do both of those things as well as the standard, computer vision approach,' said one expert. The network found to be more robust by training it to work like a part of the brain that humans rely on for object recognition.\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, it's evident that the summaries are different. \n",
    "\n",
    "However, it's challenging to determine which one is better. \n",
    "\n",
    "It's even difficult to discern whether they are significantly distinct or if there are just subtle differences between them.\n",
    "\n",
    "This is what we are going to verify now using ROUGE. When comparing the summaries of one model with those of the other, we don't get an idea of which one is better, but rather an idea of how much the summaries have changed with the fine-tuning applied to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE\n",
    "Let's install and load all the necessary libraries to conduct a ROUGE evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install evaluate\n",
    "# !pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T16:23:01.761536Z",
     "iopub.status.busy": "2024-02-11T16:23:01.761206Z",
     "iopub.status.idle": "2024-02-11T16:23:01.769416Z",
     "shell.execute_reply": "2024-02-11T16:23:01.767623Z",
     "shell.execute_reply.started": "2024-02-11T16:23:01.761507Z"
    }
   },
   "outputs": [],
   "source": [
    "#import evaluate\n",
    "#from nltk.tokenize import sent_tokenize\n",
    "#from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#With the function load of the library evaluate \n",
    "#we create a rouge_score object\n",
    "rouge_score = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating ROUGE is as simple as calling the *compute* function of the *rouge_score* object we created earlier. This function takes the texts to compare as arguments and a third value *use_stemmer*, which indicates whether it should use *stemmer* or full words for the comparison.\n",
    "\n",
    "A *stemmer* is the base of the word. Transform differents forms of a word in a same base. \n",
    "\n",
    "Some samples of steammer are: \n",
    "* Jumping -> Jump. \n",
    "* Running -> Run. \n",
    "* Cats -> Cat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_rouge_score(generated, reference):\n",
    "    \n",
    "    #We need to add '\\n' to each line before send it to ROUGE\n",
    "    generated_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in generated]\n",
    "    reference_with_newlines = [\"\\n\".join(sent_tokenize(s.strip())) for s in reference]\n",
    "    \n",
    "    return rouge_score.compute(\n",
    "        predictions=generated_with_newlines,\n",
    "        references=reference_with_newlines,\n",
    "        use_stemmer=True,\n",
    "        \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.47018752391886715,\n",
       " 'rouge2': 0.3209013209013209,\n",
       " 'rougeL': 0.34330271718331423,\n",
       " 'rougeLsum': 0.44692881745120555}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_rouge_score(summaries_small, summaries_reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a difference between the two models when performing summarization. \n",
    "\n",
    "For example, in ROUGE-1, the similarity is 47%, while in ROUGE-2, it's a 32%. This indicates that the results are different, with some similarities but differents enough. \n",
    "\n",
    "However, we still don't know which model is better since we have compared them to each other and not to a reference text. But at the very least, we know that the fine-tuning process applied to the second model has significantly altered its results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-08-07T17:21:28.296636Z",
     "iopub.status.busy": "2023-08-07T17:21:28.296173Z",
     "iopub.status.idle": "2023-08-07T17:21:28.301906Z",
     "shell.execute_reply": "2023-08-07T17:21:28.300702Z",
     "shell.execute_reply.started": "2023-08-07T17:21:28.296601Z"
    }
   },
   "source": [
    "## Comparing to a Dataset with real summaries\n",
    "\n",
    "We are going to load the Dataset cnn_dailymail. This is a well-known dataset available in the **Datasets** library, and it suits our purpose perfectly. \n",
    "\n",
    "Apart from the news, it also contains pre-existing summaries. \n",
    "\n",
    "We will compare the summaries generated by the two models we are using with those from the dataset to determine which model creates summaries that are closer to the reference ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-11T16:23:03.78564Z",
     "iopub.status.busy": "2024-02-11T16:23:03.785171Z",
     "iopub.status.idle": "2024-02-11T16:23:17.746392Z",
     "shell.execute_reply": "2024-02-11T16:23:17.744439Z",
     "shell.execute_reply.started": "2024-02-11T16:23:03.785598Z"
    }
   },
   "outputs": [],
   "source": [
    "!#pip install -q datasets==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/datasets/load.py:1486: FutureWarning: The repository for ccdv/cnn_dailymail contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/ccdv/cnn_dailymail\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c351f0942d0945a7984ca922013878a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/12.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e62affebc55466d87b569cac199a25b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/661k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3861d428af8b4897a026e8d75bc212cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8959d2d3c5e747d883d84ffca13fd430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd08a1b59fa84efcbee74f23a51f5500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cnn_dataset = load_dataset(\n",
    "    \"ccdv/cnn_dailymail\", version=\"3.0.0\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['article', 'highlights', 'id'],\n",
       "    num_rows: 3\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get just a few news to test\n",
    "sample_cnn = cnn_dataset[\"test\"].select(range(MAX_NEWS))\n",
    "\n",
    "sample_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the maximum length of the summaries to give the models the option to generate summaries of the same length, if they choose to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_length = max(len(item['highlights']) for item in sample_cnn)\n",
    "max_length = max_length + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summaries_t5_base = create_summaries(sample_cnn[\"article\"], \n",
    "                                      tokenizer_small, \n",
    "                                      model_small, \n",
    "                                      max_l=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summaries_t5_finetuned = create_summaries(sample_cnn[\"article\"], \n",
    "                                      tokenizer_reference, \n",
    "                                      model_reference, \n",
    "                                      max_l=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Get the real summaries from the cnn_dataset\n",
    "real_summaries = sample_cnn['highlights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the generated summaries alongside the reference summaries provided by the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>base</th>\n",
       "      <th>finetuned</th>\n",
       "      <th>reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>best died in hospice in Hickory, north Carolin...</td>\n",
       "      <td>Jimmie Best was \"the most constantly creative ...</td>\n",
       "      <td>James Best, who played the sheriff on \"The Duk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"it doesn't matter what anyone says, he is pre...</td>\n",
       "      <td>Dr. Anthony Moschetto's attorney calls the all...</td>\n",
       "      <td>A lawyer for Dr. Anthony Moschetto says the ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>president Barack Obama took part in a roundtab...</td>\n",
       "      <td>President Obama says climate change is a publi...</td>\n",
       "      <td>\"No challenge poses more of a public threat th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                base  \\\n",
       "0  best died in hospice in Hickory, north Carolin...   \n",
       "1  \"it doesn't matter what anyone says, he is pre...   \n",
       "2  president Barack Obama took part in a roundtab...   \n",
       "\n",
       "                                           finetuned  \\\n",
       "0  Jimmie Best was \"the most constantly creative ...   \n",
       "1  Dr. Anthony Moschetto's attorney calls the all...   \n",
       "2  President Obama says climate change is a publi...   \n",
       "\n",
       "                                           reference  \n",
       "0  James Best, who played the sheriff on \"The Duk...  \n",
       "1  A lawyer for Dr. Anthony Moschetto says the ch...  \n",
       "2  \"No challenge poses more of a public threat th...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries = pd.DataFrame.from_dict(\n",
    "        {\n",
    "            \"base\": summaries_t5_base, \n",
    "            \"finetuned\": summaries_t5_finetuned,\n",
    "            \"reference\": real_summaries,\n",
    "        }\n",
    "    )\n",
    "summaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the ROUGE scores for the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.3050834824090638,\n",
       " 'rouge2': 0.07211128178870115,\n",
       " 'rougeL': 0.2095520274299344,\n",
       " 'rougeLsum': 0.2662418008348241}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_rouge_score(summaries_t5_base, real_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': 0.31659149328289443,\n",
       " 'rouge2': 0.11065084340946411,\n",
       " 'rougeL': 0.2200203695620544,\n",
       " 'rougeLsum': 0.24877540132887144}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_rouge_score(summaries_t5_finetuned, real_summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these results, I would say that the fine-tuned model performs slightly better than the T5-Base model. It consistently achieves higher ROUGE scores in all metrics except for LSUM, where the difference is minimal.\n",
    "\n",
    "Additionally, the ROUGE metrics are quite interpretable. \n",
    "\n",
    "LSUM indicates the percentage of the longest common subsequence, regardless of word order, in relation to the total length of the text. \n",
    "\n",
    "This can be a good indicator of overall similarity between texts. However, both models have very similar LSUM scores, and the fine-tuned model has better scores in other ROUGE metrics.\n",
    "\n",
    "Personally, I would lean towards the fine-tuned model, although the difference may not be very significant."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3496946,
     "sourceId": 6104553,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30527,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
