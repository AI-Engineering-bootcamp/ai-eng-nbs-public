{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Word Embeddings \n",
    "-------\n",
    "<img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/w4-s3/embeddings.png\" alt=\"embeddings\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "---\n",
    "\n",
    "- Describe why word embeddings are a popular and powerful\n",
    "- Explain how word embeddings is a neural network\n",
    "- Understand the common training methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pop Quiz\n",
    "---\n",
    "\n",
    "Do computers prefer numbers or words?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Numbers__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What are Word Embeddings?\n",
    "-----\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "Word Embedding are a collection of algorithms that map words (strings) to numbers (vectors - arrays of floats).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why are Word Embeddings important?\n",
    "-----\n",
    "\n",
    "Turns text into a numerical form (meaningful word vectors) allows computers to better process the information.\n",
    "\n",
    "Including creating a representation that other Deep Learning and Machine Learning algorithms can use in-turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does word2vec work?\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/w4-s3/firth.png\" width=\"400\"/></center>\n",
    "\n",
    ">“You shall know a word\n",
    ">by the company it keeps”\n",
    "\n",
    "> \\- J. R. Firth 1957"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Distributional Hypothesis\n",
    "----\n",
    "\n",
    "> Words that are used and occur in the same contexts tend to have similar meanings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Example:__  \n",
    "> ... government debt problems are turning into __banking__ crises...  \n",
    "\n",
    "> ... Europe governments needs unified __banking__ regulation to replace the hodgepodge of debt regulations...\n",
    "\n",
    "The words: _government_, _regulation_ and _debt_ probably represent some aspect of __banking__ since they frequently appear near by.\n",
    "\n",
    "The words: _Pokeman_ and _tubular_ probably do __not__ represent some aspect of __banking__ since they do not frequently appear near by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does word2vec model the Distributional Hypothesis?\n",
    "---\n",
    "\n",
    "<center><img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/w4-s3/w2v_neural_net.png\" width=\"1000\"/></center>\n",
    "\n",
    "word2Vec is a __very__ simple neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once the training is complete, the output softmax layer is discarded and the remaining weights become the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"attachment:b97b7a58-3a8d-406c-a29a-2afba143df5c.png\" width=\"1000\"/></center>\n",
    "\n",
    "Note the bow-tie shape - that is is an __autoencoder__. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Autoencoders\n",
    "-------\n",
    "\n",
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/2/28/Autoencoder_structure.png\" width=\"1000\"/></center>\n",
    "\n",
    "Compress a sparse representation into a dense representation. \n",
    "\n",
    "Learns the mapping that best preserves the structure of the original space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Story time...\n",
    "----\n",
    "\n",
    "<center><img src=\"http://worldartsme.com/images/king-and-queen-clipart-1.jpg\" width=\"300\"/></center>\n",
    "\n",
    "A man and woman meet each other ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The man and woman become king and queen ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The king and queen get old and stop talking to each other. Instead, they read books and magazines ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Represent data\n",
    "corpus = \"\"\"The man and woman meet each other ...\n",
    "         The man and woman become king and queen ...\n",
    "         The king and queen get old and stop talking to each other. Instead, they read books and magazines ...\n",
    "         \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assign important words to vectors by hand\n",
    "important_words = ['queen', 'book', 'king', 'magazine', 'woman', 'man']\n",
    "\n",
    "vectors = np.array([[0.1,   0.3],  # queen\n",
    "                    [-0.5, -0.1],  # book\n",
    "                    [0.2,   0.2],  # king\n",
    "                    [-0.3, -0.2],  # magazine\n",
    "                    [-0.5,  0.4],  # car\n",
    "                    [-0.45, 0.3]]) # bike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the most important words\n",
    "plt.plot(vectors[:,0], vectors[:,1], 'o')\n",
    "for word, x, y in zip(important_words, vectors[:,0], vectors[:,1]):\n",
    "    plt.annotate(word, (x, y), size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "---\n",
    "\n",
    "How many dimensions are data represented in? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**There are 2 dimensions** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How many dimensions would we need to represent 1-hot encoded word vectors? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__5 dimensions__\n",
    "\n",
    "Typically you would use n-1 word vectors (a baseline word would be coded as all zeros). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "# Encode each word using 1-hot encoding\n",
    "{'queen':    [0, 0, 0, 0, 0],\n",
    " 'book':     [0, 0, 0, 0, 1],\n",
    " 'king':     [0, 0, 0, 1, 0],\n",
    " 'magazine': [0, 0, 1, 0, 0],\n",
    " 'woman':    [0, 1, 0, 0, 0],\n",
    " 'man':      [1, 0, 0, 0, 0],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "word2vec creates low-dimensional, dense vectors\n",
    "------\n",
    "\n",
    "In contrast to other NLP encodings which are larger and sparser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In terms of Big O space complexity, how does the dimensionality of the representation increase in word2vec vs 1-hot encoding?\n",
    "---------\n",
    "\n",
    " For example - if double the number of unique words in our dataset how many more dimensions would we need to represent the data? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Word2vec is __constant O(1)__. Word2vec always represents the data in a fixed number of dimensions.\n",
    "\n",
    "1-hot encoding is __linear O(n)__. We have to add 1 dimension for every unique word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/w4-s3/w2v_neural_net.png\" width=\"1000\"/></center>\n",
    "\n",
    "The vectors are the weights in the neural network. Each hidden node is a dimension.\n",
    "\n",
    "The dimensions are not directly interpretable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training: From random to clustering\n",
    "from IPython.display import VimeoVideo\n",
    "\n",
    "VimeoVideo(\"112168934\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The weights start random. Then group and separate during each training pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The 2 training methods for word2vec training\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1) “Skip-gram”: Predict target word, given a nearby word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2) “Continuous bag of words”: Predict target word, given a context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sentence:  \n",
    "\"Selling these fine leather jackets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Skip-gram training methodology: <br> Predict target word, given a nearby word\n",
    "----\n",
    "\n",
    "<center><img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/w4-s3/skip_gram2.png\" width=\"900\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Skip-gram example\n",
    "---\n",
    "\n",
    ">“… selling these fine leather jackets”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```\n",
    "bi-grams = {selling these, these fine, \n",
    "            fine leather, leather jackets}\n",
    "\n",
    "skip-two-bi-grams = {selling these, selling fine, selling leather, \n",
    "                    these fine, these leather, these jackets, \n",
    "                    fine leather, fine jackets, \n",
    "                    leather jackets}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Defining skip-grams\n",
    "---\n",
    "\n",
    "<img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/w4-s3/skip-gram-equation.png\" style=\"width: 400px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Skip-Gram training methodology, deep dive\n",
    "----\n",
    "\n",
    "<img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/w4-s3/skip_gram_detailed.png\" style=\"width: 400px;\"/>\n",
    "\n",
    "The target word is now at the input layer, and the context words are on the output layer.\n",
    "\n",
    "On the output layer, instead of outputing one multinomial distribution, we are outputing C multinomial distributions. Each output is computed using the same hidden to output matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Continuous bag of words (CBOW) training methodology: <br>Predict target word, given a context\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/w4-s3/cbow2.png\" width=\"900\"/></center>\n",
    "\n",
    "The context is represented as a bag of the words contained in a fixed size window around the target word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Skip-gram vs. CBOW\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/w4-s3/cbo_vs_skipgram.png\" width=\"1000\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Reference](https://fasttext.cc/docs/en/unsupervised-tutorial.html)\n",
    "\n",
    "[Detailed explanation](http://alexminnaar.com/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "CBOW vs. Skip-gram\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "CBOW is several times faster to train than the skip-gram and has slightly better accuracy for  frequent words.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Skip-gram works well with a small amount of the training data and well represents rare words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Skip-gram is the most common training methodology.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "------\n",
    "\n",
    "<center><img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/w4-s3/book.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Word2vec: Create a dense vector representation of words that models semantic meaning based on context\n",
    "- Word2Vec is popular because it is straight forward to implement and creates dense embedding vectors.\n",
    "- Word2Vec is a _relatively_ simple neural net with 1 input layer, 1 hidden layer, and 1 output layer.\n",
    "- There are 2 common training methodology: \n",
    "    1. CBOW: given context, predict word\n",
    "    2. skip-gram: given word, predict context\n",
    "- Sets you up for machine learning and Deep Learning"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
