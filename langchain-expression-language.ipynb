{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **L**ang**C**hain **E**xpression **L**anguage (LCEL) is a abstraction of some interesting Python concepts into a format that enables a \"minimalist\" code layer for building chains of LangChain components.\n",
    "\n",
    "LCEL comes with strong support for:\n",
    "\n",
    "1. Superfast development of chains.\n",
    "2. Advanced features such as streaming, async, parallel execution, and more.\n",
    "3. Easy integration with LangSmith and LangServe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install anthropic\n",
    "#pip install cohere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    langchain==0.0.345 \\\n",
    "    anthropic==0.7.7 \\\n",
    "    cohere==4.37 \\\n",
    "    docarray==0.39.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand LCEL syntax let's first build a simple chain in typical Python syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatAnthropic\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "ANTHROPIC_API_KEY= os.getenv('ANTHROPIC_API_KEY')\n",
    "COHERE_API_KEY=os.getenv('COHERE_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Give me small report about {topic}\"\n",
    ")\n",
    "model = ChatAnthropic(\n",
    "    model=\"claude-2.1\",\n",
    "    max_tokens_to_sample=512,\n",
    "    anthropic_api_key=ANTHROPIC_API_KEY\n",
    ")  # swap Anthropic for OpenAI with `ChatOpenAI` and `openai_api_key`\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In typical LangChain we would chain these together using an `LLMChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a brief report on artificial intelligence:\n",
      "\n",
      "Artificial intelligence (AI) refers to computer systems or machines that are designed to perform tasks that would otherwise require human intelligence such as visual perception, speech recognition, decision-making, and language translation. AI has seen major advances in recent years due to increased availability of data, advanced algorithms, and improvements in computing power through techniques like parallel processing and neural networks.\n",
      "\n",
      "Some key highlights about the current state of AI:\n",
      "\n",
      "- AI systems have matched or surpassed human performance in specialized tasks like playing complex games (e.g. chess, Go), image classification, and speech recognition. However, matching comprehensive human intelligence across all domains remains an aspirational goal for AI researchers.  \n",
      "\n",
      "- Machine learning, which refers to algorithms that automatically improve themselves through experience and data without explicit programming, has been one of the most promising and widely used AI techniques. Popular methods include deep learning neural networks and reinforcement learning.\n",
      "\n",
      "- AI adoption in the business world is accelerating and producing real value according to a study by MIT which found that over 70% of firms that adopted AI generated over $100 million in value. Key business uses include predictive analytics, language processing for customer service, and intelligent process automation.\n",
      "\n",
      "- However, AI also faces challenges around explainability, potential biases, and job displacement as the technology matches some human capabilities. Researchers are working to address these challenges through technical solutions as well as policy guidance.\n",
      "\n",
      "In summary, AI has shown transformative potential but still faces limitations in matching broad human cognition. With advances in research and responsible implementation, AI aims to provide major economic and societal benefits as well as augment human capabilities in the future.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "\n",
    "chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=model,\n",
    "    output_parser=output_parser\n",
    ")\n",
    "\n",
    "# and run\n",
    "out = chain.run(topic=\"Artificial Intelligence\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using LCEL the format is different, rather than relying on `Chains` we simple chain together each component using the pipe operator `|`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a brief report on artificial intelligence (AI):\n",
      "\n",
      "Introduction \n",
      "\n",
      "- Artificial intelligence refers to computer systems that are designed to perform tasks that typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\n",
      "\n",
      "- AI has advanced significantly in recent years due to increases in data availability, faster computers, and advances in machine learning techniques like deep learning.\n",
      "\n",
      "Current Applications\n",
      "\n",
      "- AI is currently used in many applications, such as:\n",
      "\n",
      "- Chatbots and virtual assistants like Alexa, Siri - interact with humans using natural language processing.\n",
      "\n",
      "- Image recognition software - identify and label objects in images and videos. Used on social media and self-driving cars.\n",
      "\n",
      "- Recommender systems - provide personalized recommendations for content on Spotify and Netflix.\n",
      "\n",
      "- Fraud detection - detect credit card and financial fraud by analyzing purchase data for anomalies. Banks save billions through AI fraud solutions.  \n",
      "\n",
      "Future Implications\n",
      "\n",
      "- AI is expected to impact both blue collar and white collar jobs through automation of tasks previously performed by humans. It may displace many types of jobs within the next 10-20 years.\n",
      "\n",
      "- At the same time, AI is expected to create new types of jobs. There is ongoing debate around ethical concerns like privacy violations and bias in AI algorithms. Governments may need to regulate aspects of AI.\n",
      "\n",
      "Conclusion\n",
      "\n",
      "- AI is transforming many industries through automation and enhanced decision making, but also raises ethical issues. Managing its rapidly accelerating progress will be crucial for employment and broader society in the coming decades.\n"
     ]
    }
   ],
   "source": [
    "lcel_chain = prompt | model | output_parser\n",
    "\n",
    "# and run\n",
    "out = lcel_chain.invoke({\"topic\": \"Artificial Intelligence\"})\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty cool, the way that this pipe operator is being used is that it is taking output from the function to the _left_ and feeding it into the function on the _right_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the Pipe Operator Works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To really understand LCEL we can take a look at how this pipe operation works. We know it takes output from the _right_ and feeds it to the _left_ — but this isn't typical Python, so how is this implemented? Let's try creating our own version of this with some simple functions.\n",
    "\n",
    "We will be using the `__or__` method within Python class objects. When we place two classes together like `chain = class_a | class_b` the Python interpreter will check if these classes contain this `__or__` method. If they do then our `|` code will be translated into `chain = class_a.__or__(class_b)`.\n",
    "\n",
    "That means both of these patterns will return the same thing:\n",
    "\n",
    "```python\n",
    "# object approach\n",
    "chain = class_a.__or__(class_b)\n",
    "chain(\"some input\")\n",
    "\n",
    "# pipe approach\n",
    "chain = class_a | class_b\n",
    "chain(\"some input\")\n",
    "\n",
    "```\n",
    "\n",
    "With that in mind, we can build a `Runnable` class that consumes a function and turns it into a function that can be chained with other functions using the pipe operator `|`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Runnable:\n",
    "    def __init__(self, func):\n",
    "        self.func = func\n",
    "\n",
    "    def __or__(self, other):\n",
    "        def chained_func(*args, **kwargs):\n",
    "            # the other func consumes the result of this func\n",
    "            return other(self.func(*args, **kwargs))\n",
    "        return Runnable(chained_func)\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        return self.func(*args, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement this to take the value `3`, add `5` (giving `8`), and multiply by `2` (giving `16`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_five(x):\n",
    "    return x + 5\n",
    "\n",
    "def multiply_by_two(x):\n",
    "    return x * 2\n",
    "\n",
    "# wrap the functions with Runnable\n",
    "add_five = Runnable(add_five)\n",
    "multiply_by_two = Runnable(multiply_by_two)\n",
    "\n",
    "# run them using the object approach\n",
    "chain = add_five.__or__(multiply_by_two)\n",
    "chain(3)  # should return 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `__or__` directly we get the correct answer, now let's try using the pipe operator `|` to chain them together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain the runnable functions together\n",
    "chain = add_five | multiply_by_two\n",
    "\n",
    "# invoke the chain\n",
    "chain(3)  # we should return 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using either method we get the same response, at it's core this is what LCEL is doing, but there is more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LCEL Deep Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand what this syntax is doing under the hood, let's explore it within the context of LCEL and see a few of the additional methods that LangChain has provided to maximize flexibility when working with LCEL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Runnables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with LCEL we may find that we need to modify the structure or values being passed between components — for this we can use _runnables_. Let's try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import CohereEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "embedding = CohereEmbeddings(\n",
    "    model=\"embed-english-light-v3.0\",\n",
    "    cohere_api_key=COHERE_API_KEY\n",
    ")\n",
    "\n",
    "vecstore_a = DocArrayInMemorySearch.from_texts(\n",
    "    [\"half the info will be here\", \"James' birthday is the 7th December\"],\n",
    "    embedding=embedding\n",
    ")\n",
    "vecstore_b = DocArrayInMemorySearch.from_texts(\n",
    "    [\"and half here\", \"James was born in 1994\"],\n",
    "    embedding=embedding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's try passing a question through to a single one of these `vecstore` objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import (\n",
    "    RunnableParallel,\n",
    "    RunnablePassthrough\n",
    ")\n",
    "\n",
    "retriever_a = vecstore_a.as_retriever()\n",
    "retriever_b = vecstore_b.as_retriever()\n",
    "\n",
    "prompt_str = \"\"\"Answer the question below using the context:\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\"context\": retriever_a, \"question\": RunnablePassthrough()}\n",
    ")\n",
    "\n",
    "chain = retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use two new objects here, `RunnableParallel` and `RunnablePassthrough`. The `RunnableParallel` object allows us to define multiple values and operations, and run them all in parallel. Here we call `retriever_a` using the input to our chain (below), and then pass the results from `retriever_a` to the next component in the `chain` via the `\"context\"` parameter.\n",
    "\n",
    "<div>\n",
    "<img src=\"./img/lecl.webp\" alt='auto' width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "The `RunnablePassthrough` object is used as a `\"passthrough\"` take takes any input to the current component (`retrieval`) and allows us to provide it in the component output via the `\"question\"` key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = chain.invoke(\"when was James born?\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have used `RunnableParallel` to create two parallel streams of information, one for `\"context\"` that is information fed in from `retriever_a`, and another for `\"question\"` which is the _passthrough_ information, ie the information that is passed through from our `chain.invoke(\"when was James born?\")` call.\n",
    "\n",
    "Using this information the chain is close to answering the question but it doesn't have enough information, it is missing the information that we have stored in `retriever_b`. Fortunately, we can have multiple parallel information streams using the `RunnableParallel` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_str = \"\"\"Answer the question below using the context:\n",
    "\n",
    "Context:\n",
    "{context_a}\n",
    "{context_b}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer: \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "\n",
    "retrieval = RunnableParallel(\n",
    "    {\n",
    "        \"context_a\": retriever_a, \"context_b\": retriever_b,\n",
    "        \"question\": RunnablePassthrough()\n",
    "    }\n",
    ")\n",
    "\n",
    "chain = retrieval | prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = chain.invoke(\"when was James born?\")\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the `RunnableParallel` object is allowing us to search with `retriever_a` _and_ `retriever_b` in parallel, ie at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnable Lambdas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `RunnableLambda` is a LangChain abstraction that allows us to turn Python functions into pipe-compatible function, similar to the `Runnable` class we created near the beginning of this notebook.\n",
    "\n",
    "Let's try it out with our earlier `add_five` and `multiply_by_two` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "# wrap the functions with RunnableLambda\n",
    "add_five = RunnableLambda(add_five)\n",
    "multiply_by_two = RunnableLambda(multiply_by_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with our earlier `Runnable` abstraction, we can use `|` operators to chain together our `RunnableLambda` abstractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = add_five | multiply_by_two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike our `Runnable` abstraction, we cannot run the `RunnableLambda` chain by calling it directly, instead we must call `chain.invoke`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we can see the same answer. Naturally we can feed custom functions into our chains using this approach. Let's try a short chain and see where we might want to insert a custom function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_str = \"Tell me an short fact about {topic}\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_str)\n",
    "\n",
    "chain = prompt | model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke({\"topic\": \"Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke({\"topic\": \"Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The returned text always includes the initial `\"Here's a short fact about ...\\n\\n\"` — let's add a function to split on double newlines `\"\\n\\n\"` and only return the fact itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_fact(x):\n",
    "    if \"\\n\\n\" in x:\n",
    "        return \"\\n\".join(x.split(\"\\n\\n\")[1:])\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "get_fact = RunnableLambda(extract_fact)\n",
    "\n",
    "chain = prompt | model | output_parser | get_fact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke({\"topic\": \"Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain.invoke({\"topic\": \"Artificial Intelligence\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're getting well formatted outputs using our final custom `get_fact` function.\n",
    "\n",
    "---\n",
    "\n",
    "That covers the essentials you need to getting started and building with the **L**ang**C**hain **E**xpression **L**anguage (LCEL). With it, we can put together chains very easily — and the current focus of the LangChain team is on further LCEL development and support.\n",
    "\n",
    "The pros and cons of LCEL are varied. Those that love it tend to focus on the minimalist code style, LCEL's support for streaming, parallel operations, and async, and LCEL's nice integration with LangChain's focus on chaining components together.\n",
    "\n",
    "There are also people that are less fond of LCEL. Typically people will point to it being yet another abstraction on top of an already very abstract library, that the syntax is confusing, against [the Zen of Python](https://peps.python.org/pep-0020/), and requires too much effort to learn new (or uncommon) syntax.\n",
    "\n",
    "Both viewpoints are entirely valid, LCEL is a very different approach — ofcourse there are major pros and major cons. In either case, if you're willing to spend some time learning the syntax, it allows us to develop very quickly, and with that in mind it's well worth learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
