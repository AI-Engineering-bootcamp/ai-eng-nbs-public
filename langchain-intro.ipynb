{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w12Jz3h_INct"
   },
   "source": [
    "# LangChain Introduction\n",
    "\n",
    "**LangChain** is a software development framework designed to simplify the creation of applications that use large language models (LLMs) like OpenAI's GPT. It's written in Python and JavaScript, and it was launched as an open-source project in October 2022 by Harrison Chase, a developer working at the machine learning startup Robust Intelligence.\n",
    "\n",
    "The framework offers a broad range of capabilities and features for developers, allowing them to harness the power of LLMs for a variety of applications. Here are some key features of LangChain:\n",
    "\n",
    "**Ease of Use**: LangChain simplifies the process of integrating LLMs into applications. This makes it easier for developers to create and deploy applications that leverage language models for a variety of tasks, including document analysis, text summarization, and chatbot functionality.\n",
    "\n",
    "**Flexibility**: LangChain includes integrations with a variety of systems and services, including Amazon, Google, and Microsoft Azure cloud storage; API wrappers for news, movie information, and weather; and support for multiple web scraping subsystems and templates.\n",
    "\n",
    "**Chain Mechanism:** At the heart of LangChain is the \"Chain\" mechanism, which is a sequence of operations that are performed on a given input. The output of one operation can be used as the input for the next, allowing for complex, multi-step processes to be created and executed easily.\n",
    "\n",
    "**Memory Functionality:** LangChain supports the concept of a memory for chain objects, allowing data to persist across multiple calls. This makes the chain a stateful object, which can be useful for certain types of applications.\n",
    "\n",
    "**Customizable Chains:** While LangChain provides many predefined chains, developers also have the flexibility to create custom chains tailored to their specific needs. This can be done by subclassing the Chain class and implementing the required methods.\n",
    "\n",
    "\n",
    "### Why we use Langchain ?\n",
    "\n",
    "Langchain is a python library that helps you interact with LLMs like ChatGPT and connect it external data and apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At its core, LangChain is a framework built around LLMs. We can use it for chatbots, Generative Question-Answering (GQA), summarization, and much more.\n",
    "\n",
    "The core idea of the library is that we can “chain” together different components to create more advanced use cases around LLMs. Chains may consist of multiple components from several modules:\n",
    "\n",
    "- Prompt templates: Prompt templates are templates for different types of prompts. Like “chatbot” style templates, ELI5 question-answering, etc\n",
    "- LLMs: Large language models like GPT-3, BLOOM, etc\n",
    "- Agents: Agents use LLMs to decide what actions should be taken. Tools like web search or calculators can be used, and all are packaged into a logical loop of operations.\n",
    "- Memory: Short-term memory, long-term memory.\n",
    "We will dive into each of these in much more detail in upcoming chapters of the LangChain handbook.\n",
    "\n",
    "For now, we’ll start with the basics behind prompt templates and LLMs. We’ll also explore two LLM options available from the library, using models from Hugging Face Hub or OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "To0cTjhBFT0V"
   },
   "source": [
    "## Generating Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "cee7L_56FdMF",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "from langchain import HuggingFaceHub\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import SimpleSequentialChain, LLMChain, LLMMathChain, TransformChain, SequentialChain\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import FewShotPromptTemplate\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import inspect\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm =   OpenAI(model_name=\"gpt-3.5-turbo-instruct\", api_key=OPENAI_API_KEY, temperature=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional approach if you want to use LLM models for free from HuggingFace - You must first install the **huggingface_hub** module and get a HuggingFace hub token. Can be slow!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id='google/flan-t5-xl',\n",
    "    model_kwargs={'temperature':1e-10}\n",
    ")\n",
    "\n",
    "# create prompt template > LLM chain\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=hub_llm\n",
    ")\n",
    "\n",
    "# ask the user question about NFL 2010\n",
    "print(llm_chain.run(question))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ghdws72ROs9I"
   },
   "source": [
    "### Prompts and Prompt templates ✏️\n",
    "\n",
    "Before understanding the concept of prompt templates it is important that you must be aware about what does prompt actually means. Prompt is simply a textual instruction which we give to a model to provide some specific output.\n",
    "\n",
    "To better understand this let us assume that we want some outline about tennis. So for this our prompt could something be like \"Write me an outline on Tennis\". But what if we want to again want to get some outline about some other sport let say cricket. In such kind of scenarios the naive approach would be to simply rewrite the prompt with updated sport.\n",
    "\n",
    "But, if you are an AI Engineer you would be aware about the concept of code reproducibility and in the above mentioned naive approach this concept is getting violated as for every different city we are forced to rewrite the entire prompts with updated sport, so to make the process of creating the prompts efficient we use prompts templates.\n",
    "\n",
    "*Prompt templates are like ready-made templates which contains contextual information about the input parameter, where input parameter is simply the input provided by the end user. The below mentioned code snipped will help you understand how we can create prompts efficiently.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure of a Prompt\n",
    "\n",
    "A prompt can consist of multiple components:\n",
    "\n",
    "* Instructions\n",
    "* External information or context\n",
    "* User input or query\n",
    "* Output indicator\n",
    "\n",
    "Not all prompts require all of these components, but often a good prompt will use two or more of them. Let's define what they all are more precisely.\n",
    "\n",
    "**Instructions** tell the model what to do, typically how it should use inputs and/or external information to produce the output we want.\n",
    "\n",
    "**External information or context** are additional information that we either manually insert into the prompt, retrieve via a vector database (long-term memory), or pull in through other means (API calls, calculations, etc).\n",
    "\n",
    "**User input or query** is typically a query directly input by the user of the system.\n",
    "\n",
    "**Output indicator** is the *beginning* of the generated text. For a model generating Python code we may put `import ` (as most Python scripts begin with a library `import`), or a chatbot may begin with `Chatbot: ` (assuming we format the chatbot script as lines of interchanging text between `User` and `Chatbot`).\n",
    "\n",
    "Each of these components should usually be placed in the order we've described them. We start with instructions, provide context (if needed), then add the user input, and finally end with the output indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "Q2-VN6LTGBIl",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter sport :  tennis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt : Write me an outline on tennis?\n"
     ]
    }
   ],
   "source": [
    "template = PromptTemplate.from_template(\"Write me an outline on {input_parameter}?\")   \n",
    "user_input = input(\"Enter sport : \")\n",
    "prompt = template.format(input_parameter=user_input)\n",
    "print(\"Prompt :\",prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter sport :  tennis\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt : input_variables=['input_parameter'] template='Write me an outline on {input_parameter}'\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate.from_template(\"Write me an outline on {input_parameter}?\")   \n",
    "user_input = input(\"Enter sport : \")\n",
    "prompt.format(input_parameter=user_input)\n",
    "# Instantiation using initializer\n",
    "prompt = PromptTemplate(input_variables=[\"input_parameter\"], template=\"Write me an outline on {input_parameter}\")\n",
    "print(\"Prompt :\",prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wouldn't typically know what the users prompt is beforehand, so we actually want to add this in. So rather than writing the prompt directly, we create a PromptTemplate with a single input variable query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fT_DGdA9VGE8"
   },
   "source": [
    "## Chains\n",
    "\n",
    "Now we are going to use a Langchain concept Chains. Chains are responsible for the entire data flow inside Langchain. As we discussed above we are passing dynamic topic input variable to OpenAI. To accommodate this we will be using a chain called LLMChain. There are a bunch of chains supported in Langchain, we will talk about them later\n",
    "\n",
    "LLMChain takes the prompt from the prompt template we created above and fills it up with the dynamic input before passing to OpenAI LLM. Let's define LLMChain below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "lszkNwMNVNiV",
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNRq_YnoQByB"
   },
   "source": [
    "Now that we have created a prompt template and a chain we can now input any topic we want. Instead of topic \"Tennis\" we can input \"Cricket\" or any other topic of your choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wUh0JGOgPxyv",
    "outputId": "cefee482-9ba6-4313-908b-555d2c511162",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "I. Introduction\n",
      "- Brief history and origin of cricket\n",
      "- Popularity and global reach of the sport\n",
      "\n",
      "II. Rules and Objectives of Cricket\n",
      "- Basic gameplay and scoring system\n",
      "- Different formats of cricket (Test, One Day, T20)\n",
      "- Role of the players and their positions on the field\n",
      "\n",
      "III. Equipment and Playing Field\n",
      "- Description of the cricket field and its dimensions\n",
      "- Essential equipment required for playing cricket (bat, ball, stumps, etc.)\n",
      "- Evolution of cricket equipment over the years\n",
      "\n",
      "IV. Skills and Techniques\n",
      "- Batting: grip, stance, and different types of shots\n",
      "- Bowling: various types of deliveries and techniques\n",
      "- Fielding: different fielding positions and techniques\n",
      "- Importance of physical fitness and training in cricket\n",
      "\n",
      "V. Major Cricket Tournaments and Leagues\n",
      "- International tournaments such as the ICC Cricket World Cup and T20 World Cup\n",
      "- Domestic leagues like the Indian Premier League (IPL) and Big Bash League (BBL)\n",
      "- Impact of these tournaments on the growth and popularity of cricket\n",
      "\n",
      "VI. Famous Players and Teams\n",
      "- Legends of the sport such as Sachin Tendulkar, Brian Lara, and Sir Don Bradman\n",
      "- Top teams in international cricket (India\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(input_parameter=\"Cricket\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVBNjz2OSzrs"
   },
   "source": [
    "Now let's extend it for a multi-input prompt. Let's generate an introductory paragraph to a blog post with variables title, audience and tone of voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "vqVEYaPPGJuM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"title\", \"audience\", \"tone\"],\n",
    "    template=\"\"\"This program will generate an introductory paragraph to a blog post given a blog title, audience, and tone of voice\n",
    "\n",
    "    Blog Title: {title}\n",
    "    Audience: {audience}\n",
    "    Tone of Voice: {tone}\"\"\",\n",
    ")\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dJgvUP0pSVat",
    "outputId": "56838235-2e98-4580-9301-6b0808f89a5e",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Welcome fellow millenials! Are you ready to discover the best activities and hidden gems that Toronto has to offer? Look no further, because I've compiled a list of must-try experiences that will make your time in this bustling city unforgettable. From rooftop patios to underground speakeasies, get ready to explore Toronto in a whole new light. So put on your adventure hats and let's dive into the best activities in Toronto, all with a lighthearted twist.\n"
     ]
    }
   ],
   "source": [
    "print(chain.run(title=\"Best Activities in Toronto\", audience=\"Millenials\", tone=\"Lighthearted\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eP3dAgTgWq-r"
   },
   "source": [
    "## Combining Chains\n",
    "\n",
    "Often we would want to do multiple tasks using GPT. For example if we wish to generate an outline for a topic and use that outline to write a blog article we need to take the outline created from the first step and copy paste and paste as input to the second step\n",
    "\n",
    "Instead we can combine chains to achieve this in a single step. We will do this using a different type of chain called Sequential Chain. A sequential chain takes the output from one chain and passes on to the next. We will cover chains in more detail later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "UqHNoQ1OXLCC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"topic\"],\n",
    "    template=\"Write me an outline on {topic}\",\n",
    ")\n",
    "\n",
    "llm = OpenAI(temperature=0.9, max_tokens=-1)\n",
    "\n",
    "chain_one = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"outline\"],\n",
    "    template=\"\"\"Write a blog article in the format of the given outline \n",
    "\n",
    "    Outline:\n",
    "    {outline}\"\"\",\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIieZmmAXaCC",
    "outputId": "ff62bf70-503d-41b0-82a5-ea54a80a0e1d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "\n",
      "I. Introduction\n",
      "    A. Explanation of Tennis\n",
      "    B. Brief history of the sport\n",
      "    C. Popularity of Tennis worldwide\n",
      "\n",
      "II. Rules and Equipment\n",
      "    A. Scoring system\n",
      "    B. Playing surface and dimensions of the court\n",
      "    C. Equipment needed (racquets, balls, etc.)\n",
      "    D. Role of the umpire and linesmen\n",
      "\n",
      "III. How to Play\n",
      "    A. Serving\n",
      "        1. Types of serves (forehand, backhand, overhead)\n",
      "        2. Proper technique\n",
      "    B. Groundstrokes\n",
      "        1. Forehand\n",
      "        2. Backhand\n",
      "    C. Volleys\n",
      "        1. Types of volleys (forehand, backhand)\n",
      "        2. When to use volleys\n",
      "    D. Strategy and tactics\n",
      "        1. Singles vs doubles\n",
      "        2. Importance of positioning and footwork\n",
      "\n",
      "IV. Types of Matches\n",
      "    A. Singles\n",
      "        1. Scoring and rules\n",
      "        2. Singles strategies and tactics\n",
      "    B. Doubles\n",
      "        1. Scoring and rules\n",
      "        2. Doubles strategies and tactics\n",
      "    C. Grand Slam tournaments\n",
      "        1. Types of Grand Slams\n",
      "        2. Top players and their records at Grand Slams\n",
      "\n",
      "V. Physical and Mental Benefits of Tennis\n",
      "    A. Physical benefits\n",
      "        1. Improved cardiovascular health\n",
      "        2. Increased strength and muscle tone\n",
      "        3. Improved coordination and flexibility\n",
      "    B. Mental benefits\n",
      "        1. Stress relief\n",
      "        2. Improved focus and concentration\n",
      "        3. Boost in self-confidence and self-esteem\n",
      "\n",
      "VI. Famous Players and Their Impact on the Sport\n",
      "    A. Men's tennis\n",
      "        1. Roger Federer\n",
      "        2. Rafael Nadal\n",
      "        3. Novak Djokovic\n",
      "    B. Women's tennis\n",
      "        1. Serena Williams\n",
      "        2. Venus Williams\n",
      "        3. Naomi Osaka\n",
      "    C. Their achievements and impact on the sport\n",
      "\n",
      "VII. Evolution of Tennis\n",
      "    A. Early forms of the sport\n",
      "    B. Introduction of professional tennis\n",
      "    C. Changes in technology and equipment\n",
      "    D. Growth of women's tennis\n",
      "\n",
      "VIII. Etiquette and Sportsmanship in Tennis\n",
      "    A. Code of conduct for players\n",
      "    B. Respect for opponents and officials\n",
      "    C. Importance of fair play and integrity\n",
      "\n",
      "IX. Conclusion\n",
      "    A. Recap of key points\n",
      "    B. Personal experiences with Tennis\n",
      "    C. Encouragement to try the sport. \u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "Tennis: A Sport for All Ages and Abilities\n",
      "\n",
      "Tennis is a popular sport that has been enjoyed by people of all ages for centuries. It requires a combination of physical and mental skills, making it an exciting and challenging game for players of all levels. In this article, we will explore the history, rules, playing techniques, and benefits of playing tennis, as well as the impact of famous players on the sport.\n",
      "\n",
      "The sport of tennis originated in France during the 12th century and was initially played as a leisure activity by noblemen. It gained popularity throughout Europe and eventually spread worldwide, becoming an Olympic sport in 1896. Today, tennis is played by millions of people in over 200 countries and is recognized as one of the most popular individual sports in the world.\n",
      "\n",
      "Tennis is played on a rectangular court, divided into two halves by a net. The objective of the game is to hit a small felt-covered ball over the net and into the opponent's side of the court using a stringed racquet. The scoring system is unique, with points given in increments of 15, and the first player to win four points wins the game. A match is won by winning two out of three sets, with each set consisting of at least six games.\n",
      "\n",
      "To play tennis, you will need a racquet, a tennis ball, and appropriate shoes. The type of surface on which the game is played can vary, with the most common being hard courts, clay courts, and grass courts. Each surface has its own characteristics, affecting the speed and bounce of the ball.\n",
      "\n",
      "Serving is the first action in a tennis game, and it determines who will serve for the rest of the game. Players can use different types of serves such as the forehand, backhand, and overhead serve, each with its own advantages. Proper technique is essential for a good serve, with players using their entire body to generate power and accuracy.\n",
      "\n",
      "Groundstrokes, which are shots played after the ball has bounced once, are an essential part of the game. The forehand and backhand are the two main types of groundstrokes, with players using different grips to produce various shots. Volleys, on the other hand, are shots played before the ball bounces, and they require quick reflexes and precise hand-eye coordination.\n",
      "\n",
      "Tennis is a game of strategy and tactics, with players constantly trying to outsmart and outmaneuver their opponents. The style of play can vary between singles and doubles, with singles being a more physically demanding game, while doubles require more teamwork and coordination. To excel in tennis, players must have good footwork and be able to anticipate their opponent's shots.\n",
      "\n",
      "There are various types of matches in tennis, including singles, doubles, and mixed doubles. The four major tournaments, also known as Grand Slams, are the Australian Open, French Open, Wimbledon, and the US Open. These tournaments attract the world's top players and offer significant prize money and ranking points.\n",
      "\n",
      "In addition to being a fun and competitive sport, tennis also offers numerous physical and mental benefits. Playing tennis regularly can improve cardiovascular health, increase strength and muscle tone, and enhance coordination and flexibility. It also provides mental benefits such as stress relief, improved focus and concentration, and a boost in self-confidence and self-esteem.\n",
      "\n",
      "The sport of tennis has produced many famous and influential players who have helped propel the sport to new heights. In the men's game, Roger Federer, Rafael Nadal, and Novak Djokovic have dominated the rankings and won multiple Grand Slams, cementing their places as legends in the sport. In women's tennis, Serena and Venus Williams, along with the current world number one Naomi Osaka, have inspired many young girls to take up the sport and break barriers in the traditionally male-dominated sport.\n",
      "\n",
      "Over the years, tennis has evolved in many ways. From its early origins as a leisure activity for the elite, it has now become a highly competitive and professional sport. Technological advancements have also played a significant role in changing the game, with modern racquets and equipment allowing for more power and control.\n",
      "\n",
      "Tennis is not just about hitting a ball back and forth across a net; it is also about sportsmanship and etiquette. Players are expected to conduct themselves with respect and integrity, showing good sportsmanship towards their opponents and officials.\n",
      "\n",
      "In conclusion, tennis is a sport that offers something for everyone, regardless of age or ability. It combines physical fitness, mental focus, and strategic thinking, making it a well-rounded and enjoyable game. Whether you are a beginner or an experienced player, there is always room for improvement in tennis. So why not grab a racquet and hit the court for a fun and challenging game of tennis?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "Tennis: A Sport for All Ages and Abilities\n",
      "\n",
      "Tennis is a popular sport that has been enjoyed by people of all ages for centuries. It requires a combination of physical and mental skills, making it an exciting and challenging game for players of all levels. In this article, we will explore the history, rules, playing techniques, and benefits of playing tennis, as well as the impact of famous players on the sport.\n",
      "\n",
      "The sport of tennis originated in France during the 12th century and was initially played as a leisure activity by noblemen. It gained popularity throughout Europe and eventually spread worldwide, becoming an Olympic sport in 1896. Today, tennis is played by millions of people in over 200 countries and is recognized as one of the most popular individual sports in the world.\n",
      "\n",
      "Tennis is played on a rectangular court, divided into two halves by a net. The objective of the game is to hit a small felt-covered ball over the net and into the opponent's side of the court using a stringed racquet. The scoring system is unique, with points given in increments of 15, and the first player to win four points wins the game. A match is won by winning two out of three sets, with each set consisting of at least six games.\n",
      "\n",
      "To play tennis, you will need a racquet, a tennis ball, and appropriate shoes. The type of surface on which the game is played can vary, with the most common being hard courts, clay courts, and grass courts. Each surface has its own characteristics, affecting the speed and bounce of the ball.\n",
      "\n",
      "Serving is the first action in a tennis game, and it determines who will serve for the rest of the game. Players can use different types of serves such as the forehand, backhand, and overhead serve, each with its own advantages. Proper technique is essential for a good serve, with players using their entire body to generate power and accuracy.\n",
      "\n",
      "Groundstrokes, which are shots played after the ball has bounced once, are an essential part of the game. The forehand and backhand are the two main types of groundstrokes, with players using different grips to produce various shots. Volleys, on the other hand, are shots played before the ball bounces, and they require quick reflexes and precise hand-eye coordination.\n",
      "\n",
      "Tennis is a game of strategy and tactics, with players constantly trying to outsmart and outmaneuver their opponents. The style of play can vary between singles and doubles, with singles being a more physically demanding game, while doubles require more teamwork and coordination. To excel in tennis, players must have good footwork and be able to anticipate their opponent's shots.\n",
      "\n",
      "There are various types of matches in tennis, including singles, doubles, and mixed doubles. The four major tournaments, also known as Grand Slams, are the Australian Open, French Open, Wimbledon, and the US Open. These tournaments attract the world's top players and offer significant prize money and ranking points.\n",
      "\n",
      "In addition to being a fun and competitive sport, tennis also offers numerous physical and mental benefits. Playing tennis regularly can improve cardiovascular health, increase strength and muscle tone, and enhance coordination and flexibility. It also provides mental benefits such as stress relief, improved focus and concentration, and a boost in self-confidence and self-esteem.\n",
      "\n",
      "The sport of tennis has produced many famous and influential players who have helped propel the sport to new heights. In the men's game, Roger Federer, Rafael Nadal, and Novak Djokovic have dominated the rankings and won multiple Grand Slams, cementing their places as legends in the sport. In women's tennis, Serena and Venus Williams, along with the current world number one Naomi Osaka, have inspired many young girls to take up the sport and break barriers in the traditionally male-dominated sport.\n",
      "\n",
      "Over the years, tennis has evolved in many ways. From its early origins as a leisure activity for the elite, it has now become a highly competitive and professional sport. Technological advancements have also played a significant role in changing the game, with modern racquets and equipment allowing for more power and control.\n",
      "\n",
      "Tennis is not just about hitting a ball back and forth across a net; it is also about sportsmanship and etiquette. Players are expected to conduct themselves with respect and integrity, showing good sportsmanship towards their opponents and officials.\n",
      "\n",
      "In conclusion, tennis is a sport that offers something for everyone, regardless of age or ability. It combines physical fitness, mental focus, and strategic thinking, making it a well-rounded and enjoyable game. Whether you are a beginner or an experienced player, there is always room for improvement in tennis. So why not grab a racquet and hit the court for a fun and challenging game of tennis?\n"
     ]
    }
   ],
   "source": [
    "overall_chain = SimpleSequentialChain(chains=[chain_one, chain_two], verbose=True)\n",
    "# Run the chain specifying only the input variable for the first chain.\n",
    "catchphrase = overall_chain.run(\"Tennis\")\n",
    "print(catchphrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Revisit the Prompt in Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prompt template classes in Langchain are built to make constructing prompts with dynamic inputs easier. Of these classes, the simplest is the PromptTemplate. We’ll test this by adding a single dynamic input to our previous prompt, the user query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Answer the question based on the context below. If the\n",
    "question cannot be answered using the information provided answer\n",
    "with \"I don't know\".\n",
    "\n",
    "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
    "Their superior performance over smaller models has made them incredibly\n",
    "useful for developers building NLP enabled applications. These models\n",
    "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
    "using the `openai` library, and via Cohere using the `cohere` library.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer: \"\"\"\n",
    "\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=template\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we can use the **format** method on our **prompt_template** to see the effect of passing a query to the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the question based on the context below. If the\n",
      "question cannot be answered using the information provided answer\n",
      "with \"I don't know\".\n",
      "\n",
      "Context: Large Language Models (LLMs) are the latest models used in NLP.\n",
      "Their superior performance over smaller models has made them incredibly\n",
      "useful for developers building NLP enabled applications. These models\n",
      "can be accessed via Hugging Face's `transformers` library, via OpenAI\n",
      "using the `openai` library, and via Cohere using the `cohere` library.\n",
      "\n",
      "Question: Which libraries and model providers offer LLMs?\n",
      "\n",
      "Answer: \n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    prompt_template.format(\n",
    "        query=\"Which libraries and model providers offer LLMs?\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.\""
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = prompt_template.format(query=\"Which libraries and model providers offer LLMs?\")\n",
    "chain = prompt_template | llm\n",
    "chain.invoke(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Shot Prompt Templates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The success of LLMs comes from their large size and ability to store “knowledge” within the model parameter, which is learned during model training. However, there are more ways to pass knowledge to an LLM. The two primary methods are:\n",
    "\n",
    "- Parametric knowledge — the knowledge mentioned above is anything that has been learned by the model during training time and is stored within the model weights (or parameters).\n",
    "- Source knowledge — any knowledge provided to the model at inference time via the input prompt.\n",
    "Langchain’s FewShotPromptTemplate caters to source knowledge input. The idea is to “train” the model on a few examples — we call this few-shot learning — and these examples are given to the model within the prompt.\n",
    "\n",
    "Few-shot learning is perfect when our model needs help understanding what we’re asking it to do. We can see this in the following example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create our examples\n",
    "examples = [ #you can have n such example of query-answer pairs\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# create a example template\n",
    "example_template = \"\"\"\n",
    "User: {query}\n",
    "AI: {answer}\n",
    "\"\"\"\n",
    "\n",
    "# create a prompt example from above template\n",
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\", \"answer\"],\n",
    "    template=example_template\n",
    ")\n",
    "\n",
    "# now break our previous prompt into a prefix and suffix\n",
    "# the prefix is our instructions\n",
    "prefix = \"\"\"The following are exerpts from conversations with an AI\n",
    "assistant. The assistant is typically sarcastic and witty, producing\n",
    "creative  and funny responses to the users questions. Here are some\n",
    "examples: \n",
    "\"\"\"\n",
    "# and the suffix our user input and output indicator\n",
    "suffix = \"\"\"\n",
    "User: {query}\n",
    "AI: \"\"\"\n",
    "\n",
    "# now create the few shot prompt template\n",
    "few_shot_prompt_template = FewShotPromptTemplate(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then pass in the examples and user query, we will get this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "\n",
      "User: What is the meaning of life?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"What is the meaning of life?\"\n",
    "\n",
    "print(few_shot_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering this, we need to balance the number of examples included and our prompt size. Our hard limit is the maximum context size, but we must also consider the cost of processing more tokens through the LLM. Fewer tokens mean a cheaper service and faster completions from the LLM.\n",
    "\n",
    "The `FewShotPromptTemplate` allows us to vary the number of examples included based on these variables. First, we create a more extensive list of examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"query\": \"How are you?\",\n",
    "        \"answer\": \"I can't complain but sometimes I still do.\"\n",
    "    }, {\n",
    "        \"query\": \"What time is it?\",\n",
    "        \"answer\": \"It's time to get a watch.\"\n",
    "    }, {\n",
    "        \"query\": \"What is the meaning of life?\",\n",
    "        \"answer\": \"42\"\n",
    "    }, {\n",
    "        \"query\": \"What is the weather like today?\",\n",
    "        \"answer\": \"Cloudy with a chance of memes.\"\n",
    "    }, {\n",
    "        \"query\": \"What is your favorite movie?\",\n",
    "        \"answer\": \"Terminator\"\n",
    "    }, {\n",
    "        \"query\": \"Who is your best friend?\",\n",
    "        \"answer\": \"Siri. We have spirited debates about the meaning of life.\"\n",
    "    }, {\n",
    "        \"query\": \"What should I do today?\",\n",
    "        \"answer\": \"Stop talking to chatbots on the internet and go outside.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, rather than passing the examples directly, we actually use a `LengthBasedExampleSelector` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.prompts.example_selector import LengthBasedExampleSelector\n",
    "\n",
    "example_selector = LengthBasedExampleSelector(\n",
    "    examples=examples,\n",
    "    example_prompt=example_prompt,\n",
    "    max_length=50  # this sets the max length that examples should be\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass our `example_selector` to the `FewShotPromptTemplate` to create a new — and dynamic — prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now create the few shot prompt template\n",
    "dynamic_prompt_template = FewShotPromptTemplate(\n",
    "    example_selector=example_selector,  # use example_selector instead of examples\n",
    "    example_prompt=example_prompt,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"query\"],\n",
    "    example_separator=\"\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we pass a shorter or longer query, we should see that the number of included examples will vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: What time is it?\n",
      "AI: It's time to get a watch.\n",
      "\n",
      "\n",
      "User: How do birds fly?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "print(dynamic_prompt_template.format(query=\"How do birds fly?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Passing a longer question will result in fewer examples being included:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following are exerpts from conversations with an AI\n",
      "assistant. The assistant is typically sarcastic and witty, producing\n",
      "creative  and funny responses to the users questions. Here are some\n",
      "examples: \n",
      "\n",
      "\n",
      "User: How are you?\n",
      "AI: I can't complain but sometimes I still do.\n",
      "\n",
      "\n",
      "User: If I am in America, and I want to call someone in another country, I'm\n",
      "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
      "what is the best way to do that?\n",
      "AI: \n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"If I am in America, and I want to call someone in another country, I'm\n",
    "thinking maybe Europe, possibly western Europe like France, Germany, or the UK,\n",
    "what is the best way to do that?\"\"\"\n",
    "\n",
    "print(dynamic_prompt_template.format(query=query))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this, we’re returning fewer examples within the prompt variable. Allowing us to limit excessive token usage and avoid errors from surpassing the maximum context window of the LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An extra utility we will use is this function that will tell us how many tokens we are using in each call. This is a good practice that is increasingly important as we use more complex tools that might make several calls to the API (like agents). It is very important to have a close control of how many tokens we are spending to avoid unsuspected expenditures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's revisit chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chains are divided in three types: Utility chains, Generic chains and Combine Documents chains. In this edition, we will focus on the first two since the third is too specific (will be covered in due course).\n",
    "\n",
    "1. Utility Chains: chains that are usually used to extract a specific answer from a llm with a very narrow purpose and are ready to be used out of the box.\n",
    "2. Generic Chains: chains that are used as building blocks for other chains but cannot be used out of the box on their own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with a simple utility chain. The `LLMMathChain` gives llms the ability to do math. Let's see how it works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pro-tip: use `verbose=True` to see what the different steps in the chain are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "What is 13 raised to the .3432 power?\u001b[32;1m\u001b[1;3m```text\n",
      "13**0.3432\n",
      "```\n",
      "...numexpr.evaluate(\"13**0.3432\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m2.4116004626599237\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Spent a total of 228 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: 2.4116004626599237'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_math = LLMMathChain.from_llm(llm=llm)\n",
    "\n",
    "# llm_math = LLMMathChain(llm=llm, verbose=True)\n",
    "\n",
    "count_tokens(llm_math, \"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what is going on here. The chain recieved a question in natural language and sent it to the llm. The llm returned a Python code which the chain compiled to give us an answer. A few questions arise.. How did the llm know that we wanted it to return Python code? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Enter prompts**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The question we send as input to the chain is not the only input that the llm recieves 😉. The input is inserted into a wider context, which gives precise instructions on how to interpret the input we send. This is called a _prompt_. Let's see what this chain's prompt is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
      "\n",
      "Question: ${{Question with math problem.}}\n",
      "```text\n",
      "${{single line mathematical expression that solves the problem}}\n",
      "```\n",
      "...numexpr.evaluate(text)...\n",
      "```output\n",
      "${{Output of running the code}}\n",
      "```\n",
      "Answer: ${{Answer}}\n",
      "\n",
      "Begin.\n",
      "\n",
      "Question: What is 37593 * 67?\n",
      "```text\n",
      "37593 * 67\n",
      "```\n",
      "...numexpr.evaluate(\"37593 * 67\")...\n",
      "```output\n",
      "2518731\n",
      "```\n",
      "Answer: 2518731\n",
      "\n",
      "Question: 37593^(1/5)\n",
      "```text\n",
      "37593**(1/5)\n",
      "```\n",
      "...numexpr.evaluate(\"37593**(1/5)\")...\n",
      "```output\n",
      "8.222831614237718\n",
      "```\n",
      "Answer: 8.222831614237718\n",
      "\n",
      "Question: {question}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(llm_math.prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok.. let's see what we got here. So, we are literally telling the llm that for complex math problems **it should not try to do math on its own** but rather it should print a Python code that will calculate the math problem instead. Probably, if we just sent the query without any context, the llm would try (and fail) to calculate this on its own. Wait! This is testable.. let's try it out! 🧐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 18 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n6.245297354'"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we set the prompt to only have the question we ask\n",
    "prompt = PromptTemplate(input_variables=['question'], template='{question}')\n",
    "llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "# we ask the llm for the answer with no context\n",
    "\n",
    "count_tokens(llm_chain, \"What is 13 raised to the .3432 power?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrong answer! Herein lies the power of prompting and one of our most important insights so far: \n",
    "\n",
    "**Insight**: _by using prompts intelligently, we can force the llm to avoid common pitfalls by explicitly and purposefully programming it to behave in a certain way._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another interesting point about this chain is that it not only runs an input through the llm but it later compiles Python code. Let's see exactly how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    def _call(\n",
      "        self,\n",
      "        inputs: Dict[str, str],\n",
      "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
      "    ) -> Dict[str, str]:\n",
      "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
      "        _run_manager.on_text(inputs[self.input_key])\n",
      "        llm_output = self.llm_chain.predict(\n",
      "            question=inputs[self.input_key],\n",
      "            stop=[\"```output\"],\n",
      "            callbacks=_run_manager.get_child(),\n",
      "        )\n",
      "        return self._process_llm_result(llm_output, _run_manager)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(inspect.getsource(llm_math._call))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see here that if the llm returns Python code we will compile it with a Python REPL* simulator. We now have the full picture of the chain: either the llm returns an answer (for simple math problems) or it returns Python code which we compile for an exact answer to harder problems. Smart!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also notice that here we get our first example of **chain composition**, a key concept behind what makes langchain special. We are using the `LLMMathChain` which in turn initializes and uses an `LLMChain` (a 'Generic Chain') when called. We can make any arbitrary number of such compositions, effectively 'chaining' many such chains to achieve highly complex and customizable behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utility chains usually follow this same basic structure: there is a prompt for constraining the llm to return a very specific type of response from a given query. We can ask the llm to create SQL queries, API calls and even create Bash commands on the fly 🔥\n",
    "\n",
    "The list continues to grow as langchain becomes more and more flexible and powerful so we encourage you to [check it out](https://python.langchain.com/v0.2/docs/how_to/) and tinker with the example notebooks that you might find interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*_A Python REPL (Read-Eval-Print Loop) is an interactive shell for executing Python code line by line_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generic chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only three Generic Chains in langchain and we will go all in to showcase them all in the same example. Let's go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have had experience of getting dirty input texts. Specifically, as we know, llms charge us by the number of tokens we use and we are not happy to pay extra when the input has extra characters. Plus its not neat 😉"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will build a custom transform function to clean the spacing of our texts. We will then use this function to build a chain where we input our text and we expect a clean text as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def transform_func(inputs: dict) -> dict:\n",
    "    text = inputs[\"text\"]\n",
    "    \n",
    "    # replace multiple new lines and multiple spaces with a single one\n",
    "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "\n",
    "    return {\"output_text\": text}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importantly, when we initialize the chain we do not send an llm as an argument. As you can imagine, not having an llm makes this chain's abilities much weaker than the example we saw earlier. However, as we will see next, combining this chain with other chains can give us highly desirable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clean_extra_spaces_chain = TransformChain(input_variables=[\"text\"], output_variables=[\"output_text\"], transform=transform_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A random text with some irregular spacing.\\n Another one here as well.'"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_extra_spaces_chain.run('A random text  with   some irregular spacing.\\n\\n\\n     Another one   here as well.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now things will get interesting.\n",
    "\n",
    "Say we want to use our chain to clean an input text and then paraphrase the input in a specific style, say a poet or a policeman. As we now know, the `TransformChain` does not use a llm so the styling will have to be done elsewhere. That's where our `LLMChain` comes in. We know about this chain already and we know that we can do cool things with smart prompting so let's take a chance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will build the prompt template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"\"\"Paraphrase this text:\n",
    "\n",
    "{output_text}\n",
    "\n",
    "In the style of a {style}.\n",
    "\n",
    "Paraphrase: \"\"\"\n",
    "prompt = PromptTemplate(input_variables=[\"style\", \"output_text\"], template=template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And next, initialize our chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "style_paraphrase_chain = LLMChain(llm=llm, prompt=prompt, output_key='final_output')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Notice that the input text in the template is called 'output_text'. Can you guess why?\n",
    "\n",
    "We are going to pass the output of the `TransformChain` to the `LLMChain`!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to combine them both to work as one integrated chain. For that we will use `SequentialChain` which is our third generic chain building block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sequential_chain = SequentialChain(chains=[clean_extra_spaces_chain, style_paraphrase_chain], input_variables=['text', 'style'], output_variables=['final_output'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our input is the langchain docs description of what chains are but dirty with some extra spaces all around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_text = \"\"\"\n",
    "Chains allow us to combine multiple \n",
    "\n",
    "\n",
    "components together to create a single, coherent application. \n",
    "\n",
    "For example, we can create a chain that takes user input,       format it with a PromptTemplate, \n",
    "\n",
    "and then passes the formatted response to an LLM. We can build more complex chains by combining     multiple chains together, or by \n",
    "\n",
    "\n",
    "combining chains with other components.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all set. Time to get creative!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 152 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Yo, check it - chains be the key, makin' dope apps is easy.\\nCombine user input, PromptTemplate, and an LLM, makin' everything crisp and clean.\\nStack multiple chains or mix 'em with other components, makin' even crazier machines.\""
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_tokens(sequential_chain, {'text': input_text, 'style': 'a 90s rapper'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on langchain-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`langchain-hub` is a sister library to `langchain`, where all the chains, agents and prompts are serialized for us to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.chains import load_chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading from langchain hub is as easy as finding the chain you want to load in the repository and then using `load_chain` with the corresponding path. We also have `load_prompt` and `initialize_agent`, but more on that later. Let's see how we can do this with our `LLMMathChain` we saw earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_math_chain = load_chain('lc://chains/llm-math/chain.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we want to change some of the configuration parameters? We can simply override it after loading:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_math_chain.verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_math_chain.verbose = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_math_chain.verbose"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
