{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/pytorch.png\" alt=\"pytorch log\" width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "# Introduction to PyTorch\n",
    "\n",
    "### What is PyTorch?\n",
    "PyTorch is an open-source machine learning library developed by Facebook's AI Research lab (FAIR). It provides a flexible framework for building and training neural networks, and it is widely used for research and production applications in areas such as computer vision, natural language processing, and reinforcement learning.\n",
    "\n",
    "### Key Features and Advantages:\n",
    "- Dynamic computation graph: PyTorch uses a dynamic computational graph approach, allowing for more flexible and intuitive model development compared to static graph frameworks.\n",
    "- Easy debugging: PyTorch's dynamic nature enables easier debugging and error handling during model development.\n",
    "- Pythonic interface: PyTorch provides a Pythonic interface that makes it easy to use and integrate with other Python libraries and frameworks.\n",
    "- GPU acceleration: PyTorch seamlessly supports GPU acceleration, allowing for efficient training and inference on NVIDIA GPUs.\n",
    "- Rich ecosystem: PyTorch has a rich ecosystem of libraries and tools for tasks such as data loading, model deployment, and visualization.\n",
    "\n",
    "### Installation and Setup:\n",
    "To install PyTorch, you can use pip, conda, or build from source. Here's how to install PyTorch using pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This command installs both PyTorch and torchvision, which is a PyTorch package containing popular datasets, model architectures, and common image transformations.\n",
    "\n",
    "For more detailed installation instructions and platform-specific considerations, refer to the official PyTorch documentation: **PyTorch Installation Guide**\n",
    "\n",
    "Once PyTorch is installed, you can start using it in your Python environment by importing the torch module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you're ready to explore the powerful capabilities of PyTorch for building and training neural networks!\n",
    "\n",
    "## 1. Tensors and Operations with PyTorch\n",
    "\n",
    "### Basics and Properties:\n",
    "- **Tensors**: Tensors are the fundamental data structure in PyTorch, similar to arrays in NumPy. They can be scalars, vectors, matrices, or higher-dimensional arrays.\n",
    "- **Properties**: \n",
    "  - Tensors have a data type (e.g., float32, int64) and a shape (e.g., [3, 4] for a 3x4 matrix).\n",
    "  - Tensors can be stored on different devices, such as CPU or GPU.\n",
    "  - PyTorch tensors support automatic differentiation using the autograd module.\n",
    "\n",
    "### Examples of Creating and Manipulating Tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a tensor from a list\n",
    "tensor1 = torch.tensor([1, 2, 3])\n",
    "\n",
    "# 2. Create a tensor of zeros\n",
    "tensor2 = torch.zeros(2, 3)\n",
    "\n",
    "# 3. Create a tensor of random values\n",
    "tensor3 = torch.randn(3, 3)\n",
    "\n",
    "# 4. Accessing and modifying tensor elements\n",
    "print(tensor3[0, 0])  # Access element at row 0, column 0\n",
    "tensor3[1, 1] = 0      # Modify element at row 1, column 1\n",
    "\n",
    "# 5. Reshaping tensors\n",
    "tensor4 = torch.arange(9).reshape(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tensor1)\n",
    "print(tensor2)\n",
    "print(tensor3)\n",
    "print(tensor4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor Operations:\n",
    "   - Arithmetic Operations: PyTorch tensors support arithmetic operations such as addition, subtraction, multiplication, and division, both element-wise and with scalar values.\n",
    "   - Indexing and Slicing: Tensors can be indexed and sliced to access specific elements or sub-tensors.\n",
    "   - Broadcasting: PyTorch automatically broadcasts tensors of different shapes during arithmetic operations, allowing for efficient element-wise operations.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "#### Arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = tensor2 + tensor3\n",
    "result = tensor1 * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Indexing and slicing\n",
    "subset = tensor4[:, :2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Broadcasting\n",
    "tensor5 = torch.tensor([1, 2, 3])\n",
    "result = tensor4 + tensor5.reshape(3, 1)  # Broadcasting tensor5 to match the shape of tensor4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Autograd: Automatic Differentiation\n",
    "\n",
    "### Introduction to Automatic Differentiation:\n",
    "Automatic differentiation is a technique used in machine learning frameworks to automatically compute gradients of functions with respect to their input variables. It plays a crucial role in training neural networks by enabling efficient optimization algorithms such as gradient descent.\n",
    "\n",
    "### How Autograd Works in PyTorch:\n",
    "PyTorch's autograd package provides automatic differentiation capabilities, allowing users to compute gradients of tensors with respect to some scalar loss function. It works by dynamically building a computational graph during the forward pass of the network and then efficiently computing gradients using the chain rule during the backward pass.\n",
    "\n",
    "### Computing Gradients with Autograd:\n",
    "To compute gradients in PyTorch, you first need to set the `requires_grad` attribute of tensors to `True` for which you want to compute gradients. Then, you perform forward pass operations, compute a scalar loss function, and call the `backward()` method on the loss tensor. This triggers the computation of gradients for all tensors with `requires_grad=True` using the chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tensors with requires_grad=True\n",
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "# Perform forward pass operations\n",
    "z = x * y\n",
    "\n",
    "# Compute scalar loss function\n",
    "loss = z**2\n",
    "\n",
    "# Compute gradients\n",
    "loss.backward()\n",
    "\n",
    "# Access gradients\n",
    "print(x.grad)  # Gradient of loss w.r.t. x\n",
    "print(y.grad)  # Gradient of loss w.r.t. y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building Neural Networks\n",
    "\n",
    "### Components of Neural Networks in PyTorch:\n",
    "Neural networks in PyTorch are composed of various components, including layers, activation functions, loss functions, and optimizers. These components work together to define the architecture, train the model, and make predictions.\n",
    "\n",
    "### Defining Network Architecture using nn.Module:\n",
    "In PyTorch, neural network architectures are defined using the `nn.Module` class. This class serves as a base class for all neural network modules and provides methods for defining and organizing the components of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "```python\n",
    "import torch.nn as nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        # Define layers and other components\n",
    "        self.fc1 = nn.Linear(in_features=784, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Define forward pass operations\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement Forward Pass and Backward Pass:\n",
    "The forward pass of a neural network is implemented in the ```forward()``` method of the ```nn.Module``` subclass. This method defines the sequence of operations that transform input data into output predictions. During training, the backward pass is automatically computed using automatic differentiation (autograd) to compute gradients of the loss function with respect to the network parameters.\n",
    "\n",
    "##### Instantiate the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork()\n",
    "\n",
    "# Forward pass\n",
    "input_data = torch.randn(64, 784)\n",
    "output_predictions = model(input_data)\n",
    "\n",
    "# Backward pass (gradient computation)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss = loss_function(output_predictions, target_labels)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Initialization and Parameter Access:\n",
    "Neural network parameters are initialized during model instantiation and can be accessed using the parameters() method. This method returns an iterator over all model parameters, which can then be used for tasks such as parameter initialization, optimization, or inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model parameters\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "model.apply(initialize_weights)\n",
    "\n",
    "# Access model parameters\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By leveraging the nn.Module class and its methods, you can easily define, train, and access neural network architectures in PyTorch.\n",
    "\n",
    "## 4. Training Models in PyTorch\n",
    "\n",
    "### Overview of the Training Process:\n",
    "Training a model in PyTorch involves iteratively feeding input data through the network, computing the loss, and updating the network parameters to minimize the loss. This process typically consists of the following steps:\n",
    "\n",
    "### Dataset and DataLoader:\n",
    "- **Dataset**: Prepare your dataset by creating a custom dataset class that inherits from `torch.utils.data.Dataset`. This class should implement methods to load and preprocess the data.\n",
    "- **DataLoader**: Use the `torch.utils.data.DataLoader` class to create batches of data from your dataset. This class provides utilities for efficient data loading and batching, including shuffling and parallelizing data loading.\n",
    "\n",
    "### Defining Loss Functions and Optimizers:\n",
    "- **Loss Functions**: Choose an appropriate loss function based on your task (e.g., classification, regression). PyTorch provides a wide range of loss functions in the `torch.nn` module, such as `nn.CrossEntropyLoss` for classification tasks and `nn.MSELoss` for regression tasks.\n",
    "- **Optimizers**: Select an optimization algorithm to update the network parameters. PyTorch provides various optimizers in the `torch.optim` module, including SGD, Adam, and RMSprop. Initialize the optimizer by passing the network parameters and specifying the learning rate.\n",
    "\n",
    "### Training Loop:\n",
    "- **Forward Pass**: Iterate over the batches of data and pass them through the network to compute the output predictions.\n",
    "- **Compute Loss**: Calculate the loss between the predicted output and the ground truth labels using the chosen loss function.\n",
    "- **Backward Pass**: Use automatic differentiation (autograd) to compute the gradients of the loss function with respect to the network parameters.\n",
    "- **Parameter Updates**: Update the network parameters using the gradients and the selected optimization algorithm.\n",
    "- **Repeat**: Iterate over the dataset multiple times (epochs) until convergence or a specified number of epochs.\n",
    "\n",
    "### Monitoring Training Progress and Evaluating Model Performance:\n",
    "- **Logging**: Track and log relevant metrics during training, such as loss and accuracy, to monitor the training progress.\n",
    "- **Validation Set**: Split your dataset into training and validation sets to evaluate the model's performance on unseen data. Use the validation set to tune hyperparameters and prevent overfitting.\n",
    "- **Evaluation Metrics**: Choose appropriate evaluation metrics based on your task (e.g., accuracy, precision, recall) to assess the model's performance on the validation set.\n",
    "\n",
    "By following these steps, you can effectively train models in PyTorch for a variety of machine learning tasks.\n",
    "\n",
    "### Example: Training Loop in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define dummy input and target data\n",
    "inputs = torch.randn(32, 10)\n",
    "targets = torch.randn(32, 1)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # Zero gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    \n",
    "    # Parameter updates\n",
    "    optimizer.step()       # Update parameters\n",
    "    \n",
    "    # Print training progress\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet above demonstrates a simple training loop in PyTorch. It includes the forward pass, backward pass (gradient computation), and parameter updates using the Stochastic Gradient Descent (SGD) optimizer. The model used is a simple neural network with one linear layer. During each epoch, the model processes the input data, computes the loss, backpropagates the gradients, and updates the model parameters.\n",
    "\n",
    "## 5. Advanced Concepts\n",
    "\n",
    "### Custom Layers and Loss Functions\n",
    "\n",
    "#### Custom Layers and Activation Functions:\n",
    "Creating custom layers and activation functions in PyTorch allows you to extend the functionality of the library and tailor neural network architectures to your specific needs. Here's how you can create custom layers and activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Custom layer example\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, output_size)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x))\n",
    "\n",
    "# Custom activation function example\n",
    "class CustomActivation(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomActivation, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)  # Custom activation function (sine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom Loss Functions:\n",
    "In addition to built-in loss functions provided by PyTorch, you can create custom loss functions tailored to specific tasks or objectives. Here's an example of how you can implement a custom loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom loss function example\n",
    "def custom_loss(output, target):\n",
    "    return torch.mean((output - target) ** 2)  # Mean squared error loss\n",
    "\n",
    "# Usage:\n",
    "criterion = custom_loss(output, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By creating custom layers, activation functions, and loss functions, you can enhance the flexibility and expressiveness of your neural network models in PyTorch, enabling you to tackle a wide range of machine learning tasks with precision and efficiency.\n",
    "\n",
    "#### Custom Optimizers\n",
    "\n",
    "Implementing custom optimization algorithms beyond built-in optimizers in PyTorch allows for fine-tuning optimization strategies to specific use cases or research objectives. Here's how you can create a custom optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Custom optimizer example\n",
    "class CustomOptimizer(optim.Optimizer):\n",
    "    def __init__(self, params, lr=0.01):\n",
    "        defaults = dict(lr=lr)\n",
    "        super(CustomOptimizer, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                grad = param.grad.data\n",
    "                param.data.add_(-group['lr'], grad)  # Custom optimization step\n",
    "        return loss\n",
    "\n",
    "# Usage:\n",
    "optimizer = CustomOptimizer(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, CustomOptimizer is a subclass of torch.optim.Optimizer that implements a custom optimization step using a specific update rule. You can define your custom update rule based on the gradients of the parameters and adjust the parameter values accordingly.\n",
    "\n",
    "By creating custom optimizers, you can experiment with novel optimization algorithms or fine-tune existing ones to improve the training process and achieve better performance for your machine learning models in PyTorch.\n",
    "\n",
    "#### GPU Acceleration\n",
    "\n",
    "Utilizing GPUs for accelerated computation in PyTorch can significantly speed up training and inference processes, especially for deep learning models with large datasets. Here's how you can move tensors and models to the GPU and back:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Example: Moving tensors to GPU\n",
    "tensor_cpu = torch.randn(32, 64)  # Create a tensor on CPU\n",
    "tensor_gpu = tensor_cpu.to(device)  # Move tensor to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, ```torch.cuda.is_available()``` checks if a GPU is available, and ```tensor.to(device)``` moves the tensor to the specified device (GPU or CPU). Similarly, you can move entire models to the GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "model.to(device)  # Move model to GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By moving both tensors and models to the GPU, you can leverage the parallel processing power of GPUs to accelerate computations and improve the efficiency of your machine learning workflows in PyTorch.\n",
    "\n",
    "#### Distributed Training\n",
    "\n",
    "Distributed training with PyTorch enables efficient utilization of multiple GPUs or even multiple machines to accelerate model training and handle larger datasets. PyTorch provides two main approaches for distributed training: `DataParallel` and `DistributedDataParallel`.\n",
    "\n",
    "##### Overview of Distributed Training:\n",
    "Distributed training involves splitting the training workload across multiple devices (GPUs or machines) and coordinating the communication between these devices to update model parameters efficiently. PyTorch supports distributed training via its `torch.distributed` package, which provides utilities for parallelism and communication.\n",
    "\n",
    "##### Multi-GPU Training with DataParallel:\n",
    "`DataParallel` is a simple and easy-to-use approach for multi-GPU training in PyTorch. It replicates the model across multiple GPUs and divides the input data into smaller batches, allowing each GPU to process a portion of the data independently. Here's how you can use `DataParallel`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap the model with DataParallel\n",
    "model = nn.DataParallel(model)\n",
    "\n",
    "Multi-GPU Training with `DistributedDataParallel`:\n",
    "DistributedDataParallel (DDP) is a more advanced approach that provides better scalability and flexibility for distributed training. It leverages the torch.distributed package to synchronize gradients and model parameters across multiple processes. Here's how you can use DistributedDataParallel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "# Initialize distributed training\n",
    "dist.init_process_group(backend='nccl', init_method='...')\n",
    "rank = dist.get_rank()\n",
    "world_size = dist.get_world_size()\n",
    "\n",
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Wrap the model with DistributedDataParallel\n",
    "model = DDP(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By utilizing `DataParallel` or `DistributedDataParalle`l, you can easily scale your PyTorch training across multiple GPUs or machines, improving training speed and efficiency.\n",
    "\n",
    "#### Model Deployment\n",
    "\n",
    "Deploying trained models for inference in production systems requires exporting the model and integrating it with deployment platforms. PyTorch provides several methods for exporting trained models and offers integration options for various deployment scenarios.\n",
    "\n",
    "##### Exporting Trained Models for Inference:\n",
    "PyTorch provides facilities for exporting trained models to formats suitable for inference in production environments, such as ONNX (Open Neural Network Exchange) and TorchScript. Here's how you can export a trained model to ONNX format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# Export the model to ONNX format\n",
    "dummy_input = torch.randn(1, 3, 224, 224)\n",
    "torch.onnx.export(model, dummy_input, \"resnet18.onnx\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Integration with Production Systems and Deployment Platforms:\n",
    "Once the model is exported, it can be integrated into production systems using various deployment platforms and frameworks, such as TensorFlow Serving, Amazon SageMaker, or Microsoft Azure ML. Here's an example of how you can deploy a PyTorch model using TensorFlow Serving:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TensorFlow Serving\n",
    "!pip install tensorflow-serving-api\n",
    "\n",
    "# Start TensorFlow Serving\n",
    "!tensorflow_model_server --port=8500 --rest_api_port=8501 --model_name=resnet18 --model_base_path=/path/to/model/directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After starting TensorFlow Serving, you can send inference requests to the server using REST API calls or gRPC requests.\n",
    "\n",
    "Alternatively, you can deploy PyTorch models using cloud-based deployment platforms, such as Amazon SageMaker or Microsoft Azure ML, which provide managed services for deploying and serving machine learning models in production environments.\n",
    "\n",
    "By exporting trained models and integrating them with production systems and deployment platforms, you can deploy PyTorch models for inference in real-world applications and scale your machine learning solutions effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
