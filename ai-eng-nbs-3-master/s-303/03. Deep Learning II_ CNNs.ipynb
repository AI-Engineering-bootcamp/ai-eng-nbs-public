{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning II: CNN (Convolutional neural network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stand Alone Reading: Introduction to CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Introduction to CNNs**:\n",
    "  - Proposed by Yann Lecun in 1989.\n",
    "  - Specialized kind of neural networks for processing data with a known, grid-like topology.\n",
    "  - Examples include time-series data (1D grid) and image data (2D grid of pixels).\n",
    "  - Effective in image recognition and classification tasks.\n",
    "\n",
    "- **Biological Inspiration**:\n",
    "  - Motivated by experiments by Hubel and Wiesel on a cat’s visual cortex.\n",
    "  - Visual cortex has regions of cells sensitive to specific regions in the visual field.\n",
    "  - Neurons in the visual cortex are connected using a layered architecture.\n",
    "  - Early layers encode primitive shapes, while later layers encode more complex shapes.\n",
    "\n",
    "- **Neocognitron and LeNet-5**:\n",
    "  - Neocognitron, proposed by Kunihiko Fukushima in 1979, was an early neural model inspired by biological principles.\n",
    "  - However, it lacked weight sharing, a key feature of modern CNNs.\n",
    "  - LeNet-5, developed by Yann Lecun in 1989, was one of the first fully convolutional architectures, incorporating weight sharing.\n",
    " - **Architectural Ideas**: \n",
    "  - CNNs combine three architectural ideas: \n",
    "    - Local receptive fields\n",
    "    - Shared weights\n",
    "    - Spatial or temporal sub-sampling\n",
    "  - These ideas ensure some degree of shift, scale, and distortion invariance.\n",
    "\n",
    "- **Image Classification Challenges**:\n",
    "  - Classifying images with ordinary fully connected feed-forward networks poses problems due to:\n",
    "    - High dimensional input data (hundreds or millions of pixels)\n",
    "    - Large number of parameters in fully connected layers (e.g., tens of thousands of weights)\n",
    "    - Increased capacity requiring larger training sets\n",
    "    - Memory requirements for storing many weights may limit hardware implementations\n",
    "\n",
    "- **Deficiency of Fully-Connected Architectures**:\n",
    "  - Fully-connected architectures ignore the topology of the input, allowing input variables to be presented in any fixed order without affecting the outcome of training.\n",
    "  - Images have a strong 2D local structure, with nearby pixels being highly correlated.\n",
    "\n",
    "- **Importance of Local Correlations**:\n",
    "  - Local correlations in images are the reason for advantages of:\n",
    "    - Extracting and combining local features before recognizing spatial or temporal objects\n",
    "    - Classifying configurations of neighboring pixels into categories such as edges, corners, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3cSy4_H6aEi"
   },
   "source": [
    "The already mentioned multilayer perceptrons represent the most general and powerful feedforward neural network model possible; they are organised in layers, such that every neuron within a layer receives its own copy of all the outputs of the previous layer as its input. This kind of model is perfect for the right kind of problem – learning from a fixed number of (more or less) unstructured parameters.\n",
    "\n",
    "> However, consider what happens to the number of parameters (weights) of such a model when being fed raw image data (f.e. a $200 \\times 200$ pixel image connected to 1024 neurons) =  40960000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlCN_f8E7OZ2"
   },
   "source": [
    "The situation quickly becomes unmanageable as image sizes grow larger, way before reaching the kind of images people usually want to work with in real applications.\n",
    "\n",
    "A common solution is to downsample the images to a size where MLPs can safely be applied. However, if we directly downsample the image, we potentially lose a wealth of information; it would be great if we would somehow be able to still do some useful (without causing an explosion in parameter count) processing of the image, prior to performing the downsampling.\n",
    "\n",
    "It turns out that there is a very efficient way of pulling this off, and it makes advantage of the structure of the information encoded within an image – it is assumed that pixels that are spatially closer together will \"cooperate\" on forming a particular feature of interest much more than ones on opposite corners of the image. Also, if a particular (smaller) feature is found to be of great importance when defining an image's label, it will be equally important if this feature was found anywhere within the image, regardless of location.\n",
    "\n",
    "Enter the convolution operator. Given a two-dimensional image, $I$, and a small matrix, $K$ of size $h \\times w$, (known as a convolution kernel), which we assume encodes a way of extracting an interesting image feature, we compute the convolved image, $I∗K$, by overlaying the kernel on top of the image in all possible ways, and recording the sum of elementwise products between the image and the kernel:\n",
    "\n",
    "$$\n",
    "output(x,y) = (I \\otimes K)(x,y) = \\sum_{m=0}^{M-1} \\sum_{n=1}^{N-1} K(m,n) I(x-n, y-m)\n",
    "$$\n",
    "\n",
    "The convolution operator forms the fundamental basis of the convolutional layer of a CNN. The layer is completely specified by a certain number of kernels, $K$, and it operates by computing the convolution of the output images of a previous layer with each of those kernels, afterwards adding the biases (one per each output image). Finally, an activation function, $\\sigma$, may be applied to all of the pixels of the output images. \n",
    "\n",
    "Typically, the input to a convolutional layer will have $d$ channels (e.g., red/green/blue in the input layer), in which case the kernels are extended to have this number of channels as well.\n",
    "\n",
    "Note that, since all we're doing here is addition and scaling of the input pixels, the kernels may be learned from a given training dataset via gradient descent, exactly as the weights of an MLP. In fact, an MLP is perfectly capable of replicating a convolutional layer, but it would require a lot more training time (and data) to learn to approximate that mode of operation.\n",
    "\n",
    "https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our first Convolution Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(all concepts are explained in this class - just see it working for now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 84854,
     "status": "ok",
     "timestamp": 1647975599801,
     "user": {
      "displayName": "Jordi Vitrià",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GgmEyLlUae4iKg7mL0rlGD0T7qj1Bpbxe-TmXfZBog=s64",
      "userId": "02382397723117011615"
     },
     "user_tz": -60
    },
    "id": "KvbTpVNS7COv",
    "outputId": "739fdd8d-96b0-4f23-ca0a-d741e46b2245"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\"\"\"\n",
    "## Prepare the data\n",
    "\"\"\"\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\"\"\"\n",
    "## Build the model\n",
    "\"\"\"\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\"\"\"\n",
    "## Train the model\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "\"\"\"\n",
    "## Evaluate the trained model\n",
    "\"\"\"\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stand Alone Reading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In mathematics, convolution is a mathematical operation on two or more than two\n",
    "functions that produces a new function expressing how the shape of one is modified by the\n",
    "others. Usually, the convolution operation used in CNNs does not correspond precisely to\n",
    "the definition of convolution as used in other fields such as engineering (signal processing)\n",
    "or pure mathematics.\n",
    "\n",
    "Suppose that you are James T. Kirk from Star Trek, and you are captured in a planet, you\n",
    "want to track the location of USS Enterprise with a laser sensor. The laser sensor provides\n",
    "a single output $x(t)$, the position of the Enterprise at time $t$. Bot $x$ and $t$ are real-valued,\n",
    "i.e., you can get a different reading from the laser sensor at any instant time. Now suppose\n",
    "that your laser sensor is somewhat noisy. \n",
    "\n",
    "To obtain a less noisy estimate of the Enterprise’s position, we would like to average together several measurements. Of course, more recent measurements are more relevant, so we will want this with a weighted average that gives more weight to recent measurements. If we apply such a weighted average operation at every moment, we obtain a new function \\(s\\) providing a smoothed estimate of the position of the spaceship:\n",
    "\n",
    "<font size=\"4\">\n",
    "\n",
    "$$ s(t) = \\int x(t_0) w(t-t_0) \\, dt_0 $$\n",
    "\n",
    "</font>\n",
    "\n",
    "This operation is called convolution, denoted as:\n",
    "\n",
    "<font size=\"4\">\n",
    "\n",
    "$$ s(t) = (x \\ast w)(t) $$\n",
    "\n",
    "</font>\n",
    "\n",
    "Of course, \\(w\\) needs to be a valid probability density function, and \\(w\\) needs to be 0 for all negative arguments, or it will look into the future, which is not okay. However, usually when we work with data on a computer, time will be discretized, and our sensor will provide data at regular intervals. In our example, it might be more realistic to assume that our laser provides a measurement once per second.\n",
    "\n",
    "The time index \\(t\\) can take on only integer values. If we now assume that \\(x\\) and \\(w\\) are defined only on integer \\(t\\), we can define the discrete convolution as:\n",
    "\n",
    "<font size=\"4\">\n",
    "\n",
    "$$ s(t) = (x \\ast w)(t) = \\sum_{n= -\\infty}^{\\infty} x(n)w(t-n) $$\n",
    "\n",
    "</font>\n",
    "\n",
    "\\(w\\) is sometimes referred to as the kernel, and the output of the convolution operator is sometimes referred to as the feature map. Often, the convolution operator can be applied over more than one axis at a time. For example, if we use a two-dimensional image \\(I\\) as our input, we would probably also use a two-dimensional kernel \\(K\\):\n",
    "\n",
    "<font size=\"4\">\n",
    "\n",
    "$$ F(i,j) = (I \\ast K)(i,j) = \\sum_{m}\\sum_{n} I(m,n)K(i-m,j-n) $$\n",
    "\n",
    "</font>\n",
    "\n",
    "Instead convolution, many neural network libraries implement a related function called the\n",
    "cross-correlation.\n",
    "<font size=\"4\">\n",
    "$$F(i,j) = (I \\ast K)(i,j)=\\sum_{m}\\sum_{n} I(i+m,j+n)K(m,n)$$\n",
    "</font>\n",
    "\n",
    "We can think cross-correlation as a Hadamard Product but after the production of input ant the kernel, output of the cross-entropy is the sum of all elements of output of the Hadamard Product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename= \"./images/cross.png\",width=500, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The convolution on engineering, signal processing is a very large subject to study. It will\n",
    "be shown more later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image illustrates a typical Convolutional Neural Network (CNN) architecture used for image classification. Starting with a 28x28 grayscale image input, the network applies a series of convolutional and pooling operations to extract features and reduce the spatial dimensions while increasing the depth of the feature maps. The final set of feature maps is flattened into a 1D vector, which is then passed through fully connected layers to perform classification across 10 possible classes. Key components include the use of ReLU activations, max-pooling layers, and a softmax output layer for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename= \"./images/mnistarch.png\",width=1000, height=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Connected Layers and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Fully Connected Layers**:\n",
    "  - Each feature in the final spatial layer is connected to each hidden state in the first fully connected layer.\n",
    "  - Functions similarly to a traditional feed-forward network.\n",
    "  - More than one fully connected layer may be used to increase computational power towards the end.\n",
    "  - Connections among these layers are structured like traditional feed-forward networks.\n",
    "  - Vast majority of parameters lie in the fully connected layers.\n",
    "  - Example: If each of two fully connected layers has 4096 hidden units, the connections between them have more than 16 million weights.\n",
    "\n",
    "- **Output Layer**:\n",
    "  - Application-specific design, such as classification.\n",
    "  - Fully connected to every neuron in the penultimate layer.\n",
    "  - Activation function (e.g., logistic, softmax, linear) chosen based on the application nature.\n",
    "\n",
    "- **Flattening the Final Feature Map**:\n",
    "  - Transforming image $W \\times H \\times D$ matrix into $1 \\times (W \\times H \\times D)$ array.\n",
    "  - First fully connected layer has $W \\times H \\times D$ neurons.\n",
    "  - Example: For handwritten digit classification with final convolution layer generating 10 feature maps of size $7\\times7$, first fully connected layer has 490 neurons, and output layer has 10 neurons for classifying digits 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename= \"./images/fullyc.png\",width=900, height=900)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename= \"./images/flattening.png\",width=500, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename= \"./images/convnet-geoff.png\",width=700, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution On Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image demonstrates a basic example of a convolution operation applied to a 5x5x1 feature image using a 2x2x1 filter. With a stride of 1 and no padding, the convolution process slides the filter across the feature image, computing the dot product at each position. This operation reduces the spatial dimensions of the feature image from 5x5 to 4x4, as shown on the right. The resulting feature image highlights the extracted features from the input image based on the applied filter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename= \"./images/convimg.png\",width=700, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image;\n",
    "Image(\"./images/3d-conv.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pooling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Convolutional Layer**:\n",
    "  - First stage in a CNN architecture.\n",
    "  - Performs convolution operation, applying filters to input data to extract features.\n",
    "  \n",
    "- **Non-linear Activation**:\n",
    "  - Second stage in a CNN architecture.\n",
    "  - Applies a non-linear activation function, such as ReLU, to introduce non-linearity into the network and enable better feature representation.\n",
    "  \n",
    "- **Pooling (Sub-sampling)**:\n",
    "  - Third stage in a CNN architecture.\n",
    "  - Modifies the output of the layer further by summarizing statistics of nearby outputs.\n",
    "  - Popular pooling functions include:\n",
    "    - **Max Pooling**: Finds the maximum element within a rectangular or square neighborhood.\n",
    "    - **Average Pooling**: Computes the average value of elements in a neighborhood.\n",
    "    - **$L^2$ Norm Pooling**: Computes the $L^2$ norm of elements in a neighborhood.\n",
    "    - **Weighted Average Pooling**: Computes the weighted average based on the distance from the central pixel.\n",
    "  - Pooling helps make the representation approximately invariant to small translations of the input.\n",
    "  - Invariance to translation means that small translations of the input result in minimal changes to the pooled outputs.\n",
    "\n",
    "You can see a example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def pool2d(im,kernel,mode='max'):\n",
    "    kernel0,kernel1 = kernel\n",
    "    feature_map = 255 * np.ones(shape=(int(im.shape[0]/kernel0)+1,\n",
    "                                       int(im.shape[1]/kernel1)+1))\n",
    "    for i in range(0,im.shape[0],kernel0):\n",
    "        for j in range(0,im.shape[1],kernel1):\n",
    "            if(mode == \"max\"):\n",
    "                feature_map[int(i/kernel0),int(j/kernel1)] = np.max(im[i:i+kernel0,j:j+kernel1])\n",
    "            if(mode == \"avg\"):\n",
    "                feature_map[int(i/kernel0),int(j/kernel1)] = im[i:i+kernel0,j:j+kernel1].mean()\n",
    "    return feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "img1 = cv2.imread('./images/lena.png',0)\n",
    "maxpool = pool2d(img1,(12,12),'max')\n",
    "avgpool = pool2d(img1,(12,12),'avg')\n",
    "fig, axs = plt.subplots(1,3,figsize=(10,10))\n",
    "axs[0].imshow(img1,cmap='gray')\n",
    "axs[1].imshow(maxpool,cmap='gray')\n",
    "axs[2].imshow(avgpool,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invarianvce to local translation can be very useful property if we care more about whether some feature is presented exactly where it is. For example, when determining whether an image contains a face, we need not know the location of the eyes with pixel-perfect accuracy. We just need to know that there is an eye on the left and right side of the face. In other contexts, it is more important to preserve the location of a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One observation is that the convolution operation reduces the size of the $(q+1)$ th layer in comparsion with the size of the $q$ th layer. This type of reduction in size is not desirable in general, because it tends to lose some information along the borders of the image (or feature map in the hidden layers). This problem can be resolved by using the padding. Kernel size in the layer $q$ denoted as $K_q$ ; in padding, one adds $(K_q-1)/2$ \"pixels\" all around the borders of the image-feature map in order to maintain the spatial footprint. Note that these pixels are really feature values in the case of the padding hidden layers. The value of each of each of these padded feature values is set to 0, irrespective of whether the input or the feature maps at the hidden layers are being padded. As a result, the spatial height and width of the input volume will both increase by $(K_q-1)$, which is exactly what they reduce by (in the output volume) after the convolution is performed. The padded portions do not contribute to the final dot product due to their values are set to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename= \"./images/padding.png\",width=700, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename= \"./images/convwpad.png\",width=400, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image;\n",
    "Image(\"./images/same_padding_no_strides.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for *half padding*, *valid padding*, and *full padding*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are other ways in which convolution can reduce the spatial footprint of the image. It is not necessary to perform the convolution at every spatial position in the layer. One can reduce the level of granularity of the convolution by using the notion of strides. When a stride of $S_q$ is used in the $q$th layer, the convolution is performed at the locations 1, $S_q+1$, $2S_q+1$, and so on along both spatial dimensions of the layer. The spatial size of the output on performing this convolution has height of $(H_q - K_q)/S_q + 1$ and a width of $(W_q - K_q)/S_q + 1$. \\\\\n",
    "It is most common to use a stride of 1, although a stride of 2 is occasionally used as well. It is rare to use strides more than 2 in normal circumstances. Even though a stride of 4 was used in the input layer of the winning architecture of the ILSVRC (ImageNet Large Scale Visual Recognition Challenge) competition of 2012, the winning entry in the subsequent year reduced the stride to 2 to improve accuracy. Larger strides can be helpful in memory-constrained settings or to reduce overfitting if the spatial resolution is unnecessarily high. Strides have the effect of rapidly increasing the receptive field of each feature in the hidden layer, while reducing the spatial footprint of the entire layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename= \"./images/stride.png\",width=400, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename= \"./images/stride2.png\",width=900, height=900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ReLU Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Nonlinear Activation Functions**:\n",
    "  - Preferred in neural networks as they allow nodes to learn more complex structures in the data.\n",
    "  - Two widely used nonlinear activation functions are:\n",
    "    - **Sigmoid Activation Function**:\n",
    "      - Transforms input into values between 0.0 and 1.0.\n",
    "      - Popular until the early 1990s.\n",
    "      - Suffers from saturation, where large values snap to 1.0 and small values snap to 0.0.\n",
    "    - **Hyperbolic Tangent (tanh) Activation Function**:\n",
    "      - Similar to sigmoid, but outputs values between -1.0 and 1.0.\n",
    "      - Preferred over sigmoid in the late 1990s and 2000s due to easier training and better performance.\n",
    "      - Also suffers from saturation and limited sensitivity.\n",
    "  \n",
    "- **ReLU Activation Function**:\n",
    "  - Applied interleaved with the convolution and pooling operations in CNNs.\n",
    "  - Element-wise operation applied per pixel in a layer.\n",
    "  - Replaces all negative pixel values in the feature map by zero.\n",
    "  - Introduces non-linearity to increase the model's ability to capture non-linear features in images.\n",
    "  - Helps overcome the linearity of convolution operations and accounts for non-linearities in real-world data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout provides us that a computationally inexpensive but powerful method of regularizing a broad family of models. To a first approximation, dropout can be thought of as a method of making bagging practical for ensembles of very many large neural networks. Bagging involves training multiple models, and evaulating multiple models on each test example. This seems impractical when each model is a large neural network, since training and evaulating such networks costly in terms of runtime and memory. It is common to use ensembles of five to ten neural networks but more than this rapidly becomes unwidely. Dropout provides an inexpensive approximation to training and evaulating a bagged ensemble of exponentially many neural netwoks.\\\\\n",
    "Specifically, dropout trains the ensemble consisting of all sub-networks that\n",
    "can be formed by removing non-output units from an underlying base network. In most modern neural networks, based on a series ofaffine transformations and nonlinearities, we can effectively remove a unit from a network by multiplying its output value by zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "Image(filename= \"./images/dropout.png\",width=700, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets review the original code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Identify where in each part of the code, the concepts were applied\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\"\"\"\n",
    "## Prepare the data\n",
    "\"\"\"\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\"\"\"\n",
    "## Build the model\n",
    "\"\"\"\n",
    "\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "        layers.Flatten(),\n",
    "\n",
    "        layers.Dropout(0.5),\n",
    "        \n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\"\"\"\n",
    "## Train the model\n",
    "\"\"\"\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n",
    "\n",
    "\"\"\"\n",
    "## Evaluate the trained model\n",
    "\"\"\"\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "NeuralNetworks.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
